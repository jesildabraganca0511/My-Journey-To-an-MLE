{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0f49455",
   "metadata": {},
   "source": [
    "DAY 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eb07f1",
   "metadata": {},
   "source": [
    "Exercise 1: Model Prediction Validator (20 min)\n",
    "Scenario: Your ML model returns predictions, but sometimes they're invalid. Build a validator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d6ef3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.0\n",
      "87.5\n",
      "87.5\n"
     ]
    }
   ],
   "source": [
    "# Model predictions with confidence scores\n",
    "predictions = [\n",
    "    {\"id\": 1, \"confidence\": 0.92, \"true_label\": 1},\n",
    "    {\"id\": 2, \"confidence\": 0.45, \"true_label\": 0},\n",
    "    {\"id\": 3, \"confidence\": 0.78, \"true_label\": 1},\n",
    "    {\"id\": 4, \"confidence\": 0.34, \"true_label\": 0},\n",
    "    {\"id\": 5, \"confidence\": 0.89, \"true_label\": 1},\n",
    "    {\"id\": 6, \"confidence\": 0.23, \"true_label\": 0},\n",
    "    {\"id\": 7, \"confidence\": 0.67, \"true_label\": 0},  # Should be misclassified at 0.5\n",
    "    {\"id\": 8, \"confidence\": 0.91, \"true_label\": 1},\n",
    "]\n",
    "\n",
    "# YOUR TASK:\n",
    "# Test three different thresholds: 0.4, 0.5, 0.6\n",
    "# For each threshold:\n",
    "#   1. Convert confidence to binary prediction (>= threshold = 1, else 0)\n",
    "#   2. Calculate how many correct predictions\n",
    "#   3. Calculate accuracy\n",
    "# Find which threshold gives best accuracy\n",
    "#\n",
    "# Expected output:\n",
    "# Threshold 0.4: 7/8 correct (87.5% accuracy)\n",
    "# Threshold 0.5: X/8 correct (X% accuracy)\n",
    "# Threshold 0.6: X/8 correct (X% accuracy)\n",
    "# Best threshold: 0.X with X% accuracy\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "thresholds = [0.4, 0.5, 0.6]\n",
    "\n",
    "for threshold in thresholds:\n",
    "    correct_count = 0\n",
    "    \n",
    "    for pred in predictions:\n",
    "        if pred[\"confidence\"]>= threshold:\n",
    "            temp_label=1\n",
    "        else:\n",
    "            temp_label=0\n",
    "\n",
    "        if pred[\"true_label\"]==temp_label:\n",
    "            correct_count+=1\n",
    "                \n",
    "        \n",
    "    acc=correct_count/len(predictions)\n",
    "    print (acc*100)  \n",
    "    \n",
    "    # Calculate and print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3620887e",
   "metadata": {},
   "source": [
    "Exercise 2: Training Data Imbalance Detector (25 min)\n",
    "MLE Context: Imbalanced datasets hurt model performance. Detect and report class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf253a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "6\n",
      "0.25\n",
      "Highly imb\n"
     ]
    }
   ],
   "source": [
    "# Training dataset labels\n",
    "training_labels = [\n",
    "    1, 0, 1, 0, 0, 0, 1, 0, 0, 0,  # 3 positive, 7 negative\n",
    "    0, 0, 0, 1, 0, 0, 0, 0, 1, 0,  # 2 positive, 8 negative\n",
    "    0, 0, 0, 0, 0, 1, 0, 0, 0, 0,  # 1 positive, 9 negative\n",
    "]\n",
    "\n",
    "# YOUR TASK:\n",
    "# 1. Count class distribution (how many 0s, how many 1s)\n",
    "# 2. Calculate class imbalance ratio (minority_class / majority_class)\n",
    "# 3. Determine if dataset is imbalanced:\n",
    "#    - Balanced: ratio > 0.8\n",
    "#    - Slightly imbalanced: 0.5 <= ratio <= 0.8\n",
    "#    - Highly imbalanced: ratio < 0.5\n",
    "# 4. Calculate what percentage minority class represents\n",
    "#\n",
    "# Expected output:\n",
    "# Total samples: 30\n",
    "# Class 0: 24 samples (80.0%)\n",
    "# Class 1: 6 samples (20.0%)\n",
    "# Imbalance ratio: 0.25\n",
    "# Status: Highly imbalanced âš ï¸\n",
    "# Recommendation: Consider resampling or class weights\n",
    "\n",
    "# YOUR CODE:\n",
    "class_0_count = 0\n",
    "class_1_count = 0\n",
    "\n",
    "for label in training_labels:\n",
    "    if label==1:\n",
    "        class_1_count+=1\n",
    "    else:\n",
    "        class_0_count+=1\n",
    "print(class_0_count)\n",
    "print(class_1_count)\n",
    "    \n",
    "if class_1_count>class_0_count:\n",
    "    ratio=class_0_count/class_1_count\n",
    "else:\n",
    "    ratio=class_1_count/class_0_count\n",
    "\n",
    "print(ratio)\n",
    "if ratio>0.8:\n",
    "    print(\"balanced\")\n",
    "elif 0.5 <= ratio <= 0.8:\n",
    "    print(\"Slightly imbalance\")\n",
    "\n",
    "else:\n",
    "    print(\"Highly imb\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a872cf64",
   "metadata": {},
   "source": [
    "Exercise 3: Feature Value Range Validator (30 min)\n",
    "MLE Context: Before training, validate that features are in expected ranges (data quality check)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f3f685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: âœ“ Valid\n",
      "Sample 1: âœ— Invalid - age: -5 (min: 0, max: 120)\n",
      "Sample 2: âœ— Invalid - credit_score: 900 (min: 300, max: 850)\n",
      "Sample 3: âœ— Invalid - income: -10000 (min: 0, max: 1000000)\n",
      "Sample 4: âœ— Invalid - age: 150 (min: 0, max: 120)\n",
      "Sample 5: âœ— Invalid - loan_amount: 600000 (min: 1000, max: 500000)\n",
      "Sample 6: âœ“ Valid\n"
     ]
    }
   ],
   "source": [
    "# Feature specifications (what's expected)\n",
    "feature_specs = {\n",
    "    \"age\": {\"min\": 0, \"max\": 120, \"type\": \"numeric\"},\n",
    "    \"income\": {\"min\": 0, \"max\": 1000000, \"type\": \"numeric\"},\n",
    "    \"credit_score\": {\"min\": 300, \"max\": 850, \"type\": \"numeric\"},\n",
    "    \"loan_amount\": {\"min\": 1000, \"max\": 500000, \"type\": \"numeric\"},\n",
    "}\n",
    "\n",
    "# Actual data samples\n",
    "samples = [\n",
    "    {\"age\": 25, \"income\": 50000, \"credit_score\": 720, \"loan_amount\": 15000},\n",
    "    {\"age\": -5, \"income\": 75000, \"credit_score\": 680, \"loan_amount\": 20000},      # Invalid age\n",
    "    {\"age\": 45, \"income\": 80000, \"credit_score\": 900, \"loan_amount\": 25000},      # Invalid credit_score\n",
    "    {\"age\": 35, \"income\": -10000, \"credit_score\": 700, \"loan_amount\": 30000},     # Invalid income\n",
    "    {\"age\": 150, \"income\": 60000, \"credit_score\": 640, \"loan_amount\": 18000},     # Invalid age\n",
    "    {\"age\": 30, \"income\": 70000, \"credit_score\": 750, \"loan_amount\": 600000},     # Invalid loan_amount\n",
    "    {\"age\": 28, \"income\": 55000, \"credit_score\": 710, \"loan_amount\": 22000},\n",
    "]\n",
    "\n",
    "# YOUR TASK:\n",
    "# 1. For each sample, validate ALL features against specs\n",
    "# 2. Track which samples are valid/invalid\n",
    "# 3. For invalid samples, list which features are out of range\n",
    "# 4. Calculate percentage of clean data\n",
    "#\n",
    "# Expected output:\n",
    "# Sample 0: âœ“ Valid\n",
    "# Sample 1: âœ— Invalid - age: -5 (min: 0, max: 120)\n",
    "# Sample 2: âœ— Invalid - credit_score: 900 (min: 300, max: 850)\n",
    "# Sample 3: âœ— Invalid - income: -10000 (min: 0, max: 1000000)\n",
    "# Sample 4: âœ— Invalid - age: 150 (min: 0, max: 120)\n",
    "# Sample 5: âœ— Invalid - loan_amount: 600000 (min: 1000, max: 500000)\n",
    "# Sample 6: âœ“ Valid\n",
    "#\n",
    "# Summary:\n",
    "# Total samples: 7\n",
    "# Valid: 2 (28.6%)\n",
    "# Invalid: 5 (71.4%)\n",
    "# Data quality: Poor - consider data cleaning\n",
    "\n",
    "# YOUR CODE:\n",
    "valid_count = 0\n",
    "invalid_count = 0\n",
    "\n",
    "for idx, sample in enumerate(samples):\n",
    "    is_valid = True\n",
    "    errors = []\n",
    "    \n",
    "    # Check each feature\n",
    "    for feature_name, value in sample.items():\n",
    "        spec = feature_specs[feature_name]\n",
    "        if (value<spec[\"min\"] or value> spec['max']):\n",
    "            is_valid=False\n",
    "            errors.append(f\"{feature_name}: {value} (min: {spec['min']}, max: {spec['max']})\")\n",
    "        \n",
    "       \n",
    "        \n",
    "    \n",
    "    if is_valid:\n",
    "        print(f\"Sample {idx}: âœ“ Valid\")\n",
    "        valid_count += 1\n",
    "    else:\n",
    "        print(f\"Sample {idx}: âœ— Invalid - {', '.join(errors)}\")\n",
    "        invalid_count += 1\n",
    "\n",
    "# Print summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef53157",
   "metadata": {},
   "source": [
    "DAY 2: Loops & Iteration (75 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1186986d",
   "metadata": {},
   "source": [
    "Exercise 1: Mini-Batch Gradient Descent Simulator (20 min)\n",
    "MLE Context: Training neural networks with mini-batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9dd46c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch 1/3 ===\n",
      "Processing batch 1/49: samples 0-31 (32 samples)\n",
      "Processing batch 2/49: samples 32-63 (32 samples)\n",
      "Processing batch 3/49: samples 64-95 (32 samples)\n",
      "Processing batch 4/49: samples 96-127 (32 samples)\n",
      "Processing batch 5/49: samples 128-159 (32 samples)\n",
      "Processing batch 6/49: samples 160-191 (32 samples)\n",
      "Processing batch 7/49: samples 192-223 (32 samples)\n",
      "Processing batch 8/49: samples 224-255 (32 samples)\n",
      "Processing batch 9/49: samples 256-287 (32 samples)\n",
      "Processing batch 10/49: samples 288-319 (32 samples)\n",
      "Processing batch 11/49: samples 320-351 (32 samples)\n",
      "Processing batch 12/49: samples 352-383 (32 samples)\n",
      "Processing batch 13/49: samples 384-415 (32 samples)\n",
      "Processing batch 14/49: samples 416-447 (32 samples)\n",
      "Processing batch 15/49: samples 448-479 (32 samples)\n",
      "Processing batch 16/49: samples 480-511 (32 samples)\n",
      "Processing batch 17/49: samples 512-543 (32 samples)\n",
      "Processing batch 18/49: samples 544-575 (32 samples)\n",
      "Processing batch 19/49: samples 576-607 (32 samples)\n",
      "Processing batch 20/49: samples 608-639 (32 samples)\n",
      "Processing batch 21/49: samples 640-671 (32 samples)\n",
      "Processing batch 22/49: samples 672-703 (32 samples)\n",
      "Processing batch 23/49: samples 704-735 (32 samples)\n",
      "Processing batch 24/49: samples 736-767 (32 samples)\n",
      "Processing batch 25/49: samples 768-799 (32 samples)\n",
      "Processing batch 26/49: samples 800-831 (32 samples)\n",
      "Processing batch 27/49: samples 832-863 (32 samples)\n",
      "Processing batch 28/49: samples 864-895 (32 samples)\n",
      "Processing batch 29/49: samples 896-927 (32 samples)\n",
      "Processing batch 30/49: samples 928-959 (32 samples)\n",
      "Processing batch 31/49: samples 960-991 (32 samples)\n",
      "Processing batch 32/49: samples 992-1023 (32 samples)\n",
      "Processing batch 33/49: samples 1024-1055 (32 samples)\n",
      "Processing batch 34/49: samples 1056-1087 (32 samples)\n",
      "Processing batch 35/49: samples 1088-1119 (32 samples)\n",
      "Processing batch 36/49: samples 1120-1151 (32 samples)\n",
      "Processing batch 37/49: samples 1152-1183 (32 samples)\n",
      "Processing batch 38/49: samples 1184-1215 (32 samples)\n",
      "Processing batch 39/49: samples 1216-1247 (32 samples)\n",
      "Processing batch 40/49: samples 1248-1279 (32 samples)\n",
      "Processing batch 41/49: samples 1280-1311 (32 samples)\n",
      "Processing batch 42/49: samples 1312-1343 (32 samples)\n",
      "Processing batch 43/49: samples 1344-1375 (32 samples)\n",
      "Processing batch 44/49: samples 1376-1407 (32 samples)\n",
      "Processing batch 45/49: samples 1408-1439 (32 samples)\n",
      "Processing batch 46/49: samples 1440-1471 (32 samples)\n",
      "Processing batch 47/49: samples 1472-1503 (32 samples)\n",
      "Processing batch 48/49: samples 1504-1535 (32 samples)\n",
      "Processing batch 49/49: samples 1536-1546 (11 samples)\n",
      "=== Epoch 2/3 ===\n",
      "Processing batch 1/49: samples 0-31 (32 samples)\n",
      "Processing batch 2/49: samples 32-63 (32 samples)\n",
      "Processing batch 3/49: samples 64-95 (32 samples)\n",
      "Processing batch 4/49: samples 96-127 (32 samples)\n",
      "Processing batch 5/49: samples 128-159 (32 samples)\n",
      "Processing batch 6/49: samples 160-191 (32 samples)\n",
      "Processing batch 7/49: samples 192-223 (32 samples)\n",
      "Processing batch 8/49: samples 224-255 (32 samples)\n",
      "Processing batch 9/49: samples 256-287 (32 samples)\n",
      "Processing batch 10/49: samples 288-319 (32 samples)\n",
      "Processing batch 11/49: samples 320-351 (32 samples)\n",
      "Processing batch 12/49: samples 352-383 (32 samples)\n",
      "Processing batch 13/49: samples 384-415 (32 samples)\n",
      "Processing batch 14/49: samples 416-447 (32 samples)\n",
      "Processing batch 15/49: samples 448-479 (32 samples)\n",
      "Processing batch 16/49: samples 480-511 (32 samples)\n",
      "Processing batch 17/49: samples 512-543 (32 samples)\n",
      "Processing batch 18/49: samples 544-575 (32 samples)\n",
      "Processing batch 19/49: samples 576-607 (32 samples)\n",
      "Processing batch 20/49: samples 608-639 (32 samples)\n",
      "Processing batch 21/49: samples 640-671 (32 samples)\n",
      "Processing batch 22/49: samples 672-703 (32 samples)\n",
      "Processing batch 23/49: samples 704-735 (32 samples)\n",
      "Processing batch 24/49: samples 736-767 (32 samples)\n",
      "Processing batch 25/49: samples 768-799 (32 samples)\n",
      "Processing batch 26/49: samples 800-831 (32 samples)\n",
      "Processing batch 27/49: samples 832-863 (32 samples)\n",
      "Processing batch 28/49: samples 864-895 (32 samples)\n",
      "Processing batch 29/49: samples 896-927 (32 samples)\n",
      "Processing batch 30/49: samples 928-959 (32 samples)\n",
      "Processing batch 31/49: samples 960-991 (32 samples)\n",
      "Processing batch 32/49: samples 992-1023 (32 samples)\n",
      "Processing batch 33/49: samples 1024-1055 (32 samples)\n",
      "Processing batch 34/49: samples 1056-1087 (32 samples)\n",
      "Processing batch 35/49: samples 1088-1119 (32 samples)\n",
      "Processing batch 36/49: samples 1120-1151 (32 samples)\n",
      "Processing batch 37/49: samples 1152-1183 (32 samples)\n",
      "Processing batch 38/49: samples 1184-1215 (32 samples)\n",
      "Processing batch 39/49: samples 1216-1247 (32 samples)\n",
      "Processing batch 40/49: samples 1248-1279 (32 samples)\n",
      "Processing batch 41/49: samples 1280-1311 (32 samples)\n",
      "Processing batch 42/49: samples 1312-1343 (32 samples)\n",
      "Processing batch 43/49: samples 1344-1375 (32 samples)\n",
      "Processing batch 44/49: samples 1376-1407 (32 samples)\n",
      "Processing batch 45/49: samples 1408-1439 (32 samples)\n",
      "Processing batch 46/49: samples 1440-1471 (32 samples)\n",
      "Processing batch 47/49: samples 1472-1503 (32 samples)\n",
      "Processing batch 48/49: samples 1504-1535 (32 samples)\n",
      "Processing batch 49/49: samples 1536-1546 (11 samples)\n",
      "=== Epoch 3/3 ===\n",
      "Processing batch 1/49: samples 0-31 (32 samples)\n",
      "Processing batch 2/49: samples 32-63 (32 samples)\n",
      "Processing batch 3/49: samples 64-95 (32 samples)\n",
      "Processing batch 4/49: samples 96-127 (32 samples)\n",
      "Processing batch 5/49: samples 128-159 (32 samples)\n",
      "Processing batch 6/49: samples 160-191 (32 samples)\n",
      "Processing batch 7/49: samples 192-223 (32 samples)\n",
      "Processing batch 8/49: samples 224-255 (32 samples)\n",
      "Processing batch 9/49: samples 256-287 (32 samples)\n",
      "Processing batch 10/49: samples 288-319 (32 samples)\n",
      "Processing batch 11/49: samples 320-351 (32 samples)\n",
      "Processing batch 12/49: samples 352-383 (32 samples)\n",
      "Processing batch 13/49: samples 384-415 (32 samples)\n",
      "Processing batch 14/49: samples 416-447 (32 samples)\n",
      "Processing batch 15/49: samples 448-479 (32 samples)\n",
      "Processing batch 16/49: samples 480-511 (32 samples)\n",
      "Processing batch 17/49: samples 512-543 (32 samples)\n",
      "Processing batch 18/49: samples 544-575 (32 samples)\n",
      "Processing batch 19/49: samples 576-607 (32 samples)\n",
      "Processing batch 20/49: samples 608-639 (32 samples)\n",
      "Processing batch 21/49: samples 640-671 (32 samples)\n",
      "Processing batch 22/49: samples 672-703 (32 samples)\n",
      "Processing batch 23/49: samples 704-735 (32 samples)\n",
      "Processing batch 24/49: samples 736-767 (32 samples)\n",
      "Processing batch 25/49: samples 768-799 (32 samples)\n",
      "Processing batch 26/49: samples 800-831 (32 samples)\n",
      "Processing batch 27/49: samples 832-863 (32 samples)\n",
      "Processing batch 28/49: samples 864-895 (32 samples)\n",
      "Processing batch 29/49: samples 896-927 (32 samples)\n",
      "Processing batch 30/49: samples 928-959 (32 samples)\n",
      "Processing batch 31/49: samples 960-991 (32 samples)\n",
      "Processing batch 32/49: samples 992-1023 (32 samples)\n",
      "Processing batch 33/49: samples 1024-1055 (32 samples)\n",
      "Processing batch 34/49: samples 1056-1087 (32 samples)\n",
      "Processing batch 35/49: samples 1088-1119 (32 samples)\n",
      "Processing batch 36/49: samples 1120-1151 (32 samples)\n",
      "Processing batch 37/49: samples 1152-1183 (32 samples)\n",
      "Processing batch 38/49: samples 1184-1215 (32 samples)\n",
      "Processing batch 39/49: samples 1216-1247 (32 samples)\n",
      "Processing batch 40/49: samples 1248-1279 (32 samples)\n",
      "Processing batch 41/49: samples 1280-1311 (32 samples)\n",
      "Processing batch 42/49: samples 1312-1343 (32 samples)\n",
      "Processing batch 43/49: samples 1344-1375 (32 samples)\n",
      "Processing batch 44/49: samples 1376-1407 (32 samples)\n",
      "Processing batch 45/49: samples 1408-1439 (32 samples)\n",
      "Processing batch 46/49: samples 1440-1471 (32 samples)\n",
      "Processing batch 47/49: samples 1472-1503 (32 samples)\n",
      "Processing batch 48/49: samples 1504-1535 (32 samples)\n",
      "Processing batch 49/49: samples 1536-1546 (11 samples)\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "total_samples = 1547  # Your dataset size\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "\n",
    "# YOUR CODE:\n",
    "import math\n",
    "\n",
    "total_batches= math.ceil(total_samples/batch_size)\n",
    "\n",
    "for i in range(1,epochs+1):\n",
    "    \n",
    "    print(f\"=== Epoch {i}/{epochs} ===\")\n",
    "    for batch_num in range(1,total_batches+1):\n",
    "        start= (batch_num-1)*32\n",
    "        end_idx = min(start + batch_size - 1, total_samples - 1)\n",
    "        samples_in_batch = end_idx - start + 1\n",
    "        print(f\"Processing batch {batch_num}/49: samples {start}-{end_idx} ({samples_in_batch} samples)\")\n",
    "\n",
    "# Print final summary\n",
    "\n",
    "# YOUR TASK:\n",
    "# Simulate training loop:\n",
    "# 1. For each epoch:\n",
    "#    - Calculate how many batches per epoch\n",
    "#    - Calculate samples in last (partial) batch\n",
    "#    - Print batch processing info\n",
    "# 2. Track total batches processed across all epochs\n",
    "# 3. Track total gradient updates (one per batch)\n",
    "#\n",
    "# Expected output:\n",
    "# === Epoch 1/3 ===\n",
    "# Processing batch 1/49: samples 0-31 (32 samples)\n",
    "# Processing batch 2/49: samples 32-63 (32 samples)\n",
    "# ...\n",
    "# Processing batch 49/49: samples 1536-1546 (11 samples) [partial batch]\n",
    "# Epoch 1 complete: 49 batches, 1547 samples\n",
    "#\n",
    "# === Epoch 2/3 ===\n",
    "# ...\n",
    "#\n",
    "# === Training Complete ===\n",
    "# Total epochs: 3\n",
    "# Total batches processed: 147\n",
    "# Total gradient updates: 147\n",
    "# Samples seen: 4641 (with repetition across epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d43afb",
   "metadata": {},
   "source": [
    "Exercise 2: Early Stopping with Patience (25 min)\n",
    "MLE Context: Stop training when validation loss stops improving (avoid overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff2de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation losses per epoch (simulated)\n",
    "val_losses = [\n",
    "    0.856, 0.734, 0.651, 0.602, 0.578,  # Improving\n",
    "    0.565, 0.559, 0.556, 0.555, 0.554,  # Small improvements\n",
    "    0.5535, 0.5534, 0.5533, 0.5532,     # Tiny improvements (should trigger early stop)\n",
    "]\n",
    "\n",
    "# Early stopping config\n",
    "patience = 3  # Stop if no improvement for 3 epochs\n",
    "min_delta = 0.01  # Minimum improvement to count as \"better\"\n",
    "\n",
    "# YOUR TASK:\n",
    "# 1. Track best validation loss seen so far\n",
    "# 2. Track epochs since last improvement\n",
    "# 3. For each epoch:\n",
    "#    - Check if current loss is better than best by at least min_delta\n",
    "#    - If yes: update best, reset patience counter\n",
    "#    - If no: increment patience counter\n",
    "# 4. Stop training if patience counter reaches limit\n",
    "# 5. Report at which epoch you would have stopped\n",
    "#\n",
    "# Expected output:\n",
    "# Epoch 1: val_loss=0.8560 (new best â¬‡ï¸)\n",
    "# Epoch 2: val_loss=0.7340 (improved by 0.1220 â¬‡ï¸)\n",
    "# Epoch 3: val_loss=0.6510 (improved by 0.0830 â¬‡ï¸)\n",
    "# Epoch 4: val_loss=0.6020 (improved by 0.0490 â¬‡ï¸)\n",
    "# Epoch 5: val_loss=0.5780 (improved by 0.0240 â¬‡ï¸)\n",
    "# Epoch 6: val_loss=0.5650 (improved by 0.0130 â¬‡ï¸)\n",
    "# Epoch 7: val_loss=0.5590 (improved by 0.0060 âž¡ï¸ patience 1/3)\n",
    "# Epoch 8: val_loss=0.5560 (improved by 0.0030 âž¡ï¸ patience 2/3)\n",
    "# Epoch 9: val_loss=0.5550 (improved by 0.0010 âž¡ï¸ patience 3/3)\n",
    "# ðŸ›‘ Early stopping triggered at epoch 9\n",
    "# Best validation loss: 0.5650 (epoch 6)\n",
    "# Saved 5 epochs of unnecessary training!\n",
    "\n",
    "# YOUR CODE:\n",
    "best_loss = float('inf')\n",
    "epochs_no_improvement = 0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch, loss in enumerate(val_losses, start=1):\n",
    "    improvement = best_loss - loss\n",
    "    \n",
    "    # Check if improvement is significant (>= min_delta)\n",
    "    # Update counters and best_loss\n",
    "    # Print status\n",
    "    # Check early stopping condition\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d056d5d8",
   "metadata": {},
   "source": [
    "Exercise 3: Cross-Validation Fold Generator (30 min)\n",
    "MLE Context: K-fold cross-validation for robust model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35591dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset indices\n",
    "dataset_size = 100\n",
    "k_folds = 5\n",
    "\n",
    "# YOUR TASK:\n",
    "# Generate k-fold cross-validation splits:\n",
    "# 1. Divide dataset into k equal folds (or as equal as possible)\n",
    "# 2. For each fold:\n",
    "#    - That fold becomes validation set\n",
    "#    - All other folds become training set\n",
    "# 3. Print train/val splits for each fold\n",
    "# 4. Ensure no data leakage (no overlap between train/val in same fold)\n",
    "#\n",
    "# Expected output:\n",
    "# Fold 1/5:\n",
    "#   Train indices: [20-99] (80 samples)\n",
    "#   Val indices: [0-19] (20 samples)\n",
    "#\n",
    "# Fold 2/5:\n",
    "#   Train indices: [0-19, 40-99] (80 samples)\n",
    "#   Val indices: [20-39] (20 samples)\n",
    "#\n",
    "# Fold 3/5:\n",
    "#   Train indices: [0-39, 60-99] (80 samples)\n",
    "#   Val indices: [40-59] (20 samples)\n",
    "#\n",
    "# ... etc\n",
    "#\n",
    "# Verification:\n",
    "# âœ“ Each sample used for validation exactly once\n",
    "# âœ“ Each sample used for training exactly 4 times\n",
    "# âœ“ No overlap between train/val in any fold\n",
    "\n",
    "# YOUR CODE:\n",
    "fold_size = dataset_size // k_folds\n",
    "all_indices = list(range(dataset_size))\n",
    "\n",
    "for fold in range(k_folds):\n",
    "    # Calculate validation indices for this fold\n",
    "    val_start = fold * fold_size\n",
    "    val_end = val_start + fold_size if fold < k_folds - 1 else dataset_size\n",
    "    \n",
    "    val_indices = list(range(val_start, val_end))\n",
    "    \n",
    "    # Train indices = all indices except validation\n",
    "    train_indices = [i for i in all_indices if i not in val_indices]\n",
    "    \n",
    "    # Print fold info\n",
    "    print(f\"\\nFold {fold + 1}/{k_folds}:\")\n",
    "    print(f\"  Train indices: {len(train_indices)} samples\")\n",
    "    print(f\"  Val indices: {len(val_indices)} samples\")\n",
    "    \n",
    "    # Bonus: verify no overlap\n",
    "    overlap = set(train_indices) & set(val_indices)\n",
    "    if overlap:\n",
    "        print(f\"  âš ï¸ ERROR: Overlap detected: {overlap}\")\n",
    "    else:\n",
    "        print(f\"  âœ“ No overlap\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
