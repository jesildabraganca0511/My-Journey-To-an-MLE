{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0f49455",
   "metadata": {},
   "source": [
    "DAY 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eb07f1",
   "metadata": {},
   "source": [
    "Exercise 1: Model Prediction Validator (20 min)\n",
    "Scenario: Your ML model returns predictions, but sometimes they're invalid. Build a validator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d6ef3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.0\n",
      "87.5\n",
      "87.5\n"
     ]
    }
   ],
   "source": [
    "# Model predictions with confidence scores\n",
    "predictions = [\n",
    "    {\"id\": 1, \"confidence\": 0.92, \"true_label\": 1},\n",
    "    {\"id\": 2, \"confidence\": 0.45, \"true_label\": 0},\n",
    "    {\"id\": 3, \"confidence\": 0.78, \"true_label\": 1},\n",
    "    {\"id\": 4, \"confidence\": 0.34, \"true_label\": 0},\n",
    "    {\"id\": 5, \"confidence\": 0.89, \"true_label\": 1},\n",
    "    {\"id\": 6, \"confidence\": 0.23, \"true_label\": 0},\n",
    "    {\"id\": 7, \"confidence\": 0.67, \"true_label\": 0},  # Should be misclassified at 0.5\n",
    "    {\"id\": 8, \"confidence\": 0.91, \"true_label\": 1},\n",
    "]\n",
    "\n",
    "# YOUR TASK:\n",
    "# Test three different thresholds: 0.4, 0.5, 0.6\n",
    "# For each threshold:\n",
    "#   1. Convert confidence to binary prediction (>= threshold = 1, else 0)\n",
    "#   2. Calculate how many correct predictions\n",
    "#   3. Calculate accuracy\n",
    "# Find which threshold gives best accuracy\n",
    "#\n",
    "# Expected output:\n",
    "# Threshold 0.4: 7/8 correct (87.5% accuracy)\n",
    "# Threshold 0.5: X/8 correct (X% accuracy)\n",
    "# Threshold 0.6: X/8 correct (X% accuracy)\n",
    "# Best threshold: 0.X with X% accuracy\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "thresholds = [0.4, 0.5, 0.6]\n",
    "\n",
    "for threshold in thresholds:\n",
    "    correct_count = 0\n",
    "    \n",
    "    for pred in predictions:\n",
    "        if pred[\"confidence\"]>= threshold:\n",
    "            temp_label=1\n",
    "        else:\n",
    "            temp_label=0\n",
    "\n",
    "        if pred[\"true_label\"]==temp_label:\n",
    "            correct_count+=1\n",
    "                \n",
    "        \n",
    "    acc=correct_count/len(predictions)\n",
    "    print (acc*100)  \n",
    "    \n",
    "    # Calculate and print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3620887e",
   "metadata": {},
   "source": [
    "Exercise 2: Training Data Imbalance Detector (25 min)\n",
    "MLE Context: Imbalanced datasets hurt model performance. Detect and report class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf253a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "6\n",
      "0.25\n",
      "Highly imb\n"
     ]
    }
   ],
   "source": [
    "# Training dataset labels\n",
    "training_labels = [\n",
    "    1, 0, 1, 0, 0, 0, 1, 0, 0, 0,  # 3 positive, 7 negative\n",
    "    0, 0, 0, 1, 0, 0, 0, 0, 1, 0,  # 2 positive, 8 negative\n",
    "    0, 0, 0, 0, 0, 1, 0, 0, 0, 0,  # 1 positive, 9 negative\n",
    "]\n",
    "\n",
    "# YOUR TASK:\n",
    "# 1. Count class distribution (how many 0s, how many 1s)\n",
    "# 2. Calculate class imbalance ratio (minority_class / majority_class)\n",
    "# 3. Determine if dataset is imbalanced:\n",
    "#    - Balanced: ratio > 0.8\n",
    "#    - Slightly imbalanced: 0.5 <= ratio <= 0.8\n",
    "#    - Highly imbalanced: ratio < 0.5\n",
    "# 4. Calculate what percentage minority class represents\n",
    "#\n",
    "# Expected output:\n",
    "# Total samples: 30\n",
    "# Class 0: 24 samples (80.0%)\n",
    "# Class 1: 6 samples (20.0%)\n",
    "# Imbalance ratio: 0.25\n",
    "# Status: Highly imbalanced âš ï¸\n",
    "# Recommendation: Consider resampling or class weights\n",
    "\n",
    "# YOUR CODE:\n",
    "class_0_count = 0\n",
    "class_1_count = 0\n",
    "\n",
    "for label in training_labels:\n",
    "    if label==1:\n",
    "        class_1_count+=1\n",
    "    else:\n",
    "        class_0_count+=1\n",
    "print(class_0_count)\n",
    "print(class_1_count)\n",
    "    \n",
    "if class_1_count>class_0_count:\n",
    "    ratio=class_0_count/class_1_count\n",
    "else:\n",
    "    ratio=class_1_count/class_0_count\n",
    "\n",
    "print(ratio)\n",
    "if ratio>0.8:\n",
    "    print(\"balanced\")\n",
    "elif 0.5 <= ratio <= 0.8:\n",
    "    print(\"Slightly imbalance\")\n",
    "\n",
    "else:\n",
    "    print(\"Highly imb\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a872cf64",
   "metadata": {},
   "source": [
    "Exercise 3: Feature Value Range Validator (30 min)\n",
    "MLE Context: Before training, validate that features are in expected ranges (data quality check)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f3f685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: âœ“ Valid\n",
      "Sample 1: âœ— Invalid - age: -5 (min: 0, max: 120)\n",
      "Sample 2: âœ— Invalid - credit_score: 900 (min: 300, max: 850)\n",
      "Sample 3: âœ— Invalid - income: -10000 (min: 0, max: 1000000)\n",
      "Sample 4: âœ— Invalid - age: 150 (min: 0, max: 120)\n",
      "Sample 5: âœ— Invalid - loan_amount: 600000 (min: 1000, max: 500000)\n",
      "Sample 6: âœ“ Valid\n"
     ]
    }
   ],
   "source": [
    "# Feature specifications (what's expected)\n",
    "feature_specs = {\n",
    "    \"age\": {\"min\": 0, \"max\": 120, \"type\": \"numeric\"},\n",
    "    \"income\": {\"min\": 0, \"max\": 1000000, \"type\": \"numeric\"},\n",
    "    \"credit_score\": {\"min\": 300, \"max\": 850, \"type\": \"numeric\"},\n",
    "    \"loan_amount\": {\"min\": 1000, \"max\": 500000, \"type\": \"numeric\"},\n",
    "}\n",
    "\n",
    "# Actual data samples\n",
    "samples = [\n",
    "    {\"age\": 25, \"income\": 50000, \"credit_score\": 720, \"loan_amount\": 15000},\n",
    "    {\"age\": -5, \"income\": 75000, \"credit_score\": 680, \"loan_amount\": 20000},      # Invalid age\n",
    "    {\"age\": 45, \"income\": 80000, \"credit_score\": 900, \"loan_amount\": 25000},      # Invalid credit_score\n",
    "    {\"age\": 35, \"income\": -10000, \"credit_score\": 700, \"loan_amount\": 30000},     # Invalid income\n",
    "    {\"age\": 150, \"income\": 60000, \"credit_score\": 640, \"loan_amount\": 18000},     # Invalid age\n",
    "    {\"age\": 30, \"income\": 70000, \"credit_score\": 750, \"loan_amount\": 600000},     # Invalid loan_amount\n",
    "    {\"age\": 28, \"income\": 55000, \"credit_score\": 710, \"loan_amount\": 22000},\n",
    "]\n",
    "\n",
    "# YOUR TASK:\n",
    "# 1. For each sample, validate ALL features against specs\n",
    "# 2. Track which samples are valid/invalid\n",
    "# 3. For invalid samples, list which features are out of range\n",
    "# 4. Calculate percentage of clean data\n",
    "#\n",
    "# Expected output:\n",
    "# Sample 0: âœ“ Valid\n",
    "# Sample 1: âœ— Invalid - age: -5 (min: 0, max: 120)\n",
    "# Sample 2: âœ— Invalid - credit_score: 900 (min: 300, max: 850)\n",
    "# Sample 3: âœ— Invalid - income: -10000 (min: 0, max: 1000000)\n",
    "# Sample 4: âœ— Invalid - age: 150 (min: 0, max: 120)\n",
    "# Sample 5: âœ— Invalid - loan_amount: 600000 (min: 1000, max: 500000)\n",
    "# Sample 6: âœ“ Valid\n",
    "#\n",
    "# Summary:\n",
    "# Total samples: 7\n",
    "# Valid: 2 (28.6%)\n",
    "# Invalid: 5 (71.4%)\n",
    "# Data quality: Poor - consider data cleaning\n",
    "\n",
    "# YOUR CODE:\n",
    "valid_count = 0\n",
    "invalid_count = 0\n",
    "\n",
    "for idx, sample in enumerate(samples):\n",
    "    is_valid = True\n",
    "    errors = []\n",
    "    \n",
    "    # Check each feature\n",
    "    for feature_name, value in sample.items():\n",
    "        spec = feature_specs[feature_name]\n",
    "        if (value<spec[\"min\"] or value> spec['max']):\n",
    "            is_valid=False\n",
    "            errors.append(f\"{feature_name}: {value} (min: {spec['min']}, max: {spec['max']})\")\n",
    "        \n",
    "       \n",
    "        \n",
    "    \n",
    "    if is_valid:\n",
    "        print(f\"Sample {idx}: âœ“ Valid\")\n",
    "        valid_count += 1\n",
    "    else:\n",
    "        print(f\"Sample {idx}: âœ— Invalid - {', '.join(errors)}\")\n",
    "        invalid_count += 1\n",
    "\n",
    "# Print summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef53157",
   "metadata": {},
   "source": [
    "DAY 2: Loops & Iteration (75 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1186986d",
   "metadata": {},
   "source": [
    "Exercise 1: Mini-Batch Gradient Descent Simulator (20 min)\n",
    "MLE Context: Training neural networks with mini-batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9dd46c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch 1/3 ===\n",
      "Processing batch 1/49: samples 0-31 (32 samples)\n",
      "Processing batch 2/49: samples 32-63 (32 samples)\n",
      "Processing batch 3/49: samples 64-95 (32 samples)\n",
      "Processing batch 4/49: samples 96-127 (32 samples)\n",
      "Processing batch 5/49: samples 128-159 (32 samples)\n",
      "Processing batch 6/49: samples 160-191 (32 samples)\n",
      "Processing batch 7/49: samples 192-223 (32 samples)\n",
      "Processing batch 8/49: samples 224-255 (32 samples)\n",
      "Processing batch 9/49: samples 256-287 (32 samples)\n",
      "Processing batch 10/49: samples 288-319 (32 samples)\n",
      "Processing batch 11/49: samples 320-351 (32 samples)\n",
      "Processing batch 12/49: samples 352-383 (32 samples)\n",
      "Processing batch 13/49: samples 384-415 (32 samples)\n",
      "Processing batch 14/49: samples 416-447 (32 samples)\n",
      "Processing batch 15/49: samples 448-479 (32 samples)\n",
      "Processing batch 16/49: samples 480-511 (32 samples)\n",
      "Processing batch 17/49: samples 512-543 (32 samples)\n",
      "Processing batch 18/49: samples 544-575 (32 samples)\n",
      "Processing batch 19/49: samples 576-607 (32 samples)\n",
      "Processing batch 20/49: samples 608-639 (32 samples)\n",
      "Processing batch 21/49: samples 640-671 (32 samples)\n",
      "Processing batch 22/49: samples 672-703 (32 samples)\n",
      "Processing batch 23/49: samples 704-735 (32 samples)\n",
      "Processing batch 24/49: samples 736-767 (32 samples)\n",
      "Processing batch 25/49: samples 768-799 (32 samples)\n",
      "Processing batch 26/49: samples 800-831 (32 samples)\n",
      "Processing batch 27/49: samples 832-863 (32 samples)\n",
      "Processing batch 28/49: samples 864-895 (32 samples)\n",
      "Processing batch 29/49: samples 896-927 (32 samples)\n",
      "Processing batch 30/49: samples 928-959 (32 samples)\n",
      "Processing batch 31/49: samples 960-991 (32 samples)\n",
      "Processing batch 32/49: samples 992-1023 (32 samples)\n",
      "Processing batch 33/49: samples 1024-1055 (32 samples)\n",
      "Processing batch 34/49: samples 1056-1087 (32 samples)\n",
      "Processing batch 35/49: samples 1088-1119 (32 samples)\n",
      "Processing batch 36/49: samples 1120-1151 (32 samples)\n",
      "Processing batch 37/49: samples 1152-1183 (32 samples)\n",
      "Processing batch 38/49: samples 1184-1215 (32 samples)\n",
      "Processing batch 39/49: samples 1216-1247 (32 samples)\n",
      "Processing batch 40/49: samples 1248-1279 (32 samples)\n",
      "Processing batch 41/49: samples 1280-1311 (32 samples)\n",
      "Processing batch 42/49: samples 1312-1343 (32 samples)\n",
      "Processing batch 43/49: samples 1344-1375 (32 samples)\n",
      "Processing batch 44/49: samples 1376-1407 (32 samples)\n",
      "Processing batch 45/49: samples 1408-1439 (32 samples)\n",
      "Processing batch 46/49: samples 1440-1471 (32 samples)\n",
      "Processing batch 47/49: samples 1472-1503 (32 samples)\n",
      "Processing batch 48/49: samples 1504-1535 (32 samples)\n",
      "Processing batch 49/49: samples 1536-1546 (11 samples)\n",
      "=== Epoch 2/3 ===\n",
      "Processing batch 1/49: samples 0-31 (32 samples)\n",
      "Processing batch 2/49: samples 32-63 (32 samples)\n",
      "Processing batch 3/49: samples 64-95 (32 samples)\n",
      "Processing batch 4/49: samples 96-127 (32 samples)\n",
      "Processing batch 5/49: samples 128-159 (32 samples)\n",
      "Processing batch 6/49: samples 160-191 (32 samples)\n",
      "Processing batch 7/49: samples 192-223 (32 samples)\n",
      "Processing batch 8/49: samples 224-255 (32 samples)\n",
      "Processing batch 9/49: samples 256-287 (32 samples)\n",
      "Processing batch 10/49: samples 288-319 (32 samples)\n",
      "Processing batch 11/49: samples 320-351 (32 samples)\n",
      "Processing batch 12/49: samples 352-383 (32 samples)\n",
      "Processing batch 13/49: samples 384-415 (32 samples)\n",
      "Processing batch 14/49: samples 416-447 (32 samples)\n",
      "Processing batch 15/49: samples 448-479 (32 samples)\n",
      "Processing batch 16/49: samples 480-511 (32 samples)\n",
      "Processing batch 17/49: samples 512-543 (32 samples)\n",
      "Processing batch 18/49: samples 544-575 (32 samples)\n",
      "Processing batch 19/49: samples 576-607 (32 samples)\n",
      "Processing batch 20/49: samples 608-639 (32 samples)\n",
      "Processing batch 21/49: samples 640-671 (32 samples)\n",
      "Processing batch 22/49: samples 672-703 (32 samples)\n",
      "Processing batch 23/49: samples 704-735 (32 samples)\n",
      "Processing batch 24/49: samples 736-767 (32 samples)\n",
      "Processing batch 25/49: samples 768-799 (32 samples)\n",
      "Processing batch 26/49: samples 800-831 (32 samples)\n",
      "Processing batch 27/49: samples 832-863 (32 samples)\n",
      "Processing batch 28/49: samples 864-895 (32 samples)\n",
      "Processing batch 29/49: samples 896-927 (32 samples)\n",
      "Processing batch 30/49: samples 928-959 (32 samples)\n",
      "Processing batch 31/49: samples 960-991 (32 samples)\n",
      "Processing batch 32/49: samples 992-1023 (32 samples)\n",
      "Processing batch 33/49: samples 1024-1055 (32 samples)\n",
      "Processing batch 34/49: samples 1056-1087 (32 samples)\n",
      "Processing batch 35/49: samples 1088-1119 (32 samples)\n",
      "Processing batch 36/49: samples 1120-1151 (32 samples)\n",
      "Processing batch 37/49: samples 1152-1183 (32 samples)\n",
      "Processing batch 38/49: samples 1184-1215 (32 samples)\n",
      "Processing batch 39/49: samples 1216-1247 (32 samples)\n",
      "Processing batch 40/49: samples 1248-1279 (32 samples)\n",
      "Processing batch 41/49: samples 1280-1311 (32 samples)\n",
      "Processing batch 42/49: samples 1312-1343 (32 samples)\n",
      "Processing batch 43/49: samples 1344-1375 (32 samples)\n",
      "Processing batch 44/49: samples 1376-1407 (32 samples)\n",
      "Processing batch 45/49: samples 1408-1439 (32 samples)\n",
      "Processing batch 46/49: samples 1440-1471 (32 samples)\n",
      "Processing batch 47/49: samples 1472-1503 (32 samples)\n",
      "Processing batch 48/49: samples 1504-1535 (32 samples)\n",
      "Processing batch 49/49: samples 1536-1546 (11 samples)\n",
      "=== Epoch 3/3 ===\n",
      "Processing batch 1/49: samples 0-31 (32 samples)\n",
      "Processing batch 2/49: samples 32-63 (32 samples)\n",
      "Processing batch 3/49: samples 64-95 (32 samples)\n",
      "Processing batch 4/49: samples 96-127 (32 samples)\n",
      "Processing batch 5/49: samples 128-159 (32 samples)\n",
      "Processing batch 6/49: samples 160-191 (32 samples)\n",
      "Processing batch 7/49: samples 192-223 (32 samples)\n",
      "Processing batch 8/49: samples 224-255 (32 samples)\n",
      "Processing batch 9/49: samples 256-287 (32 samples)\n",
      "Processing batch 10/49: samples 288-319 (32 samples)\n",
      "Processing batch 11/49: samples 320-351 (32 samples)\n",
      "Processing batch 12/49: samples 352-383 (32 samples)\n",
      "Processing batch 13/49: samples 384-415 (32 samples)\n",
      "Processing batch 14/49: samples 416-447 (32 samples)\n",
      "Processing batch 15/49: samples 448-479 (32 samples)\n",
      "Processing batch 16/49: samples 480-511 (32 samples)\n",
      "Processing batch 17/49: samples 512-543 (32 samples)\n",
      "Processing batch 18/49: samples 544-575 (32 samples)\n",
      "Processing batch 19/49: samples 576-607 (32 samples)\n",
      "Processing batch 20/49: samples 608-639 (32 samples)\n",
      "Processing batch 21/49: samples 640-671 (32 samples)\n",
      "Processing batch 22/49: samples 672-703 (32 samples)\n",
      "Processing batch 23/49: samples 704-735 (32 samples)\n",
      "Processing batch 24/49: samples 736-767 (32 samples)\n",
      "Processing batch 25/49: samples 768-799 (32 samples)\n",
      "Processing batch 26/49: samples 800-831 (32 samples)\n",
      "Processing batch 27/49: samples 832-863 (32 samples)\n",
      "Processing batch 28/49: samples 864-895 (32 samples)\n",
      "Processing batch 29/49: samples 896-927 (32 samples)\n",
      "Processing batch 30/49: samples 928-959 (32 samples)\n",
      "Processing batch 31/49: samples 960-991 (32 samples)\n",
      "Processing batch 32/49: samples 992-1023 (32 samples)\n",
      "Processing batch 33/49: samples 1024-1055 (32 samples)\n",
      "Processing batch 34/49: samples 1056-1087 (32 samples)\n",
      "Processing batch 35/49: samples 1088-1119 (32 samples)\n",
      "Processing batch 36/49: samples 1120-1151 (32 samples)\n",
      "Processing batch 37/49: samples 1152-1183 (32 samples)\n",
      "Processing batch 38/49: samples 1184-1215 (32 samples)\n",
      "Processing batch 39/49: samples 1216-1247 (32 samples)\n",
      "Processing batch 40/49: samples 1248-1279 (32 samples)\n",
      "Processing batch 41/49: samples 1280-1311 (32 samples)\n",
      "Processing batch 42/49: samples 1312-1343 (32 samples)\n",
      "Processing batch 43/49: samples 1344-1375 (32 samples)\n",
      "Processing batch 44/49: samples 1376-1407 (32 samples)\n",
      "Processing batch 45/49: samples 1408-1439 (32 samples)\n",
      "Processing batch 46/49: samples 1440-1471 (32 samples)\n",
      "Processing batch 47/49: samples 1472-1503 (32 samples)\n",
      "Processing batch 48/49: samples 1504-1535 (32 samples)\n",
      "Processing batch 49/49: samples 1536-1546 (11 samples)\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "total_samples = 1547  # Your dataset size\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "\n",
    "# YOUR CODE:\n",
    "import math\n",
    "\n",
    "total_batches= math.ceil(total_samples/batch_size)\n",
    "\n",
    "for i in range(1,epochs+1):\n",
    "    \n",
    "    print(f\"=== Epoch {i}/{epochs} ===\")\n",
    "    for batch_num in range(1,total_batches+1):\n",
    "        start= (batch_num-1)*32\n",
    "        end_idx = min(start + batch_size - 1, total_samples - 1)\n",
    "        samples_in_batch = end_idx - start + 1\n",
    "        print(f\"Processing batch {batch_num}/49: samples {start}-{end_idx} ({samples_in_batch} samples)\")\n",
    "\n",
    "# Print final summary\n",
    "\n",
    "# YOUR TASK:\n",
    "# Simulate training loop:\n",
    "# 1. For each epoch:\n",
    "#    - Calculate how many batches per epoch\n",
    "#    - Calculate samples in last (partial) batch\n",
    "#    - Print batch processing info\n",
    "# 2. Track total batches processed across all epochs\n",
    "# 3. Track total gradient updates (one per batch)\n",
    "#\n",
    "# Expected output:\n",
    "# === Epoch 1/3 ===\n",
    "# Processing batch 1/49: samples 0-31 (32 samples)\n",
    "# Processing batch 2/49: samples 32-63 (32 samples)\n",
    "# ...\n",
    "# Processing batch 49/49: samples 1536-1546 (11 samples) [partial batch]\n",
    "# Epoch 1 complete: 49 batches, 1547 samples\n",
    "#\n",
    "# === Epoch 2/3 ===\n",
    "# ...\n",
    "#\n",
    "# === Training Complete ===\n",
    "# Total epochs: 3\n",
    "# Total batches processed: 147\n",
    "# Total gradient updates: 147\n",
    "# Samples seen: 4641 (with repetition across epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d43afb",
   "metadata": {},
   "source": [
    "Exercise 2: Early Stopping with Patience (25 min)\n",
    "MLE Context: Stop training when validation loss stops improving (avoid overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ff2de8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: val_loss=0.86 (new best â¬‡ï¸)\n",
      "Epoch2 : val_loss =0.73 (improved by 0.122\n",
      "Epoch3 : val_loss =0.65 (improved by 0.083\n",
      "Epoch4 : val_loss =0.60 (improved by 0.049\n",
      "Epoch5 : val_loss =0.58 (improved by 0.024\n",
      "Epoch6 : val_loss =0.56 (improved by 0.013\n",
      "Epoch 7: val_loss=0.5590 (improved by 0.0060 âž¡ï¸ patience 1/3)\n",
      "Epoch 8: val_loss=0.5560 (improved by 0.0090 âž¡ï¸ patience 2/3)\n",
      "Epoch 9: val_loss=0.5550 (improved by 0.0100 âž¡ï¸ patience 3/3)\n",
      "\n",
      "ðŸ›‘ Early stopping triggered at epoch 9\n"
     ]
    }
   ],
   "source": [
    "# Validation losses per epoch (simulated)\n",
    "val_losses = [\n",
    "    0.856, 0.734, 0.651, 0.602, 0.578,  # Improving\n",
    "    0.565, 0.559, 0.556, 0.555, 0.554,  # Small improvements\n",
    "    0.5535, 0.5534, 0.5533, 0.5532,     # Tiny improvements (should trigger early stop)\n",
    "]\n",
    "\n",
    "# Early stopping config\n",
    "patience = 3  # Stop if no improvement for 3 epochs\n",
    "min_delta = 0.01  # Minimum improvement to count as \"better\"\n",
    "\n",
    "# YOUR TASK:\n",
    "# 1. Track best validation loss seen so far\n",
    "# 2. Track epochs since last improvement\n",
    "# 3. For each epoch:\n",
    "#    - Check if current loss is better than best by at least min_delta\n",
    "#    - If yes: update best, reset patience counter\n",
    "#    - If no: increment patience counter\n",
    "# 4. Stop training if patience counter reaches limit\n",
    "# 5. Report at which epoch you would have stopped\n",
    "\n",
    "\n",
    "# YOUR CODE:\n",
    "best_loss =  float('inf')\n",
    "epochs_no_improvement = 0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch, loss in enumerate(val_losses, start=1):\n",
    "    \n",
    "    improvement = best_loss- loss\n",
    "    if improvement>= min_delta:\n",
    "        best_loss= loss\n",
    "        best_epoch=epoch\n",
    "        if epoch==1:\n",
    "            print(f\"Epoch {epoch}: val_loss={loss:.2f} (new best â¬‡ï¸)\")\n",
    "        else:\n",
    "            print(f\"Epoch{epoch} : val_loss ={loss:.2f} (improved by {improvement:.3f}\")\n",
    "    else:\n",
    "        epochs_no_improvement+=1\n",
    "        print(f\"Epoch {epoch}: val_loss={loss:.4f} \"\n",
    "              f\"(improved by {improvement:.4f} âž¡ï¸ \"\n",
    "              f\"patience {epochs_no_improvement}/{patience})\")\n",
    "        if epochs_no_improvement>=patience:\n",
    "            print(f\"\\nðŸ›‘ Early stopping triggered at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "        \n",
    "    \n",
    "    # Check if improvement is significant (>= min_delta)\n",
    "    # Update counters and best_loss\n",
    "    # Print status\n",
    "    # Check early stopping condition\n",
    "    \n",
    "\n",
    "\n",
    "#\n",
    "# Expected output:\n",
    "# Epoch 1: val_loss=0.8560 (new best â¬‡ï¸)\n",
    "# Epoch 2: val_loss=0.7340 (improved by 0.1220 â¬‡ï¸)\n",
    "# Epoch 3: val_loss=0.6510 (improved by 0.0830 â¬‡ï¸)\n",
    "# Epoch 4: val_loss=0.6020 (improved by 0.0490 â¬‡ï¸)\n",
    "# Epoch 5: val_loss=0.5780 (improved by 0.0240 â¬‡ï¸)\n",
    "# Epoch 6: val_loss=0.5650 (improved by 0.0130 â¬‡ï¸)\n",
    "# Epoch 7: val_loss=0.5590 (improved by 0.0060 âž¡ï¸ patience 1/3)\n",
    "# Epoch 8: val_loss=0.5560 (improved by 0.0030 âž¡ï¸ patience 2/3)\n",
    "# Epoch 9: val_loss=0.5550 (improved by 0.0010 âž¡ï¸ patience 3/3)\n",
    "# ðŸ›‘ Early stopping triggered at epoch 9\n",
    "# Best validation loss: 0.5650 (epoch 6)\n",
    "# Saved 5 epochs of unnecessary training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d056d5d8",
   "metadata": {},
   "source": [
    "Exercise 3: Cross-Validation Fold Generator (30 min)\n",
    "MLE Context: K-fold cross-validation for robust model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "35591dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
      "validation [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "training [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
      "validation [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38]\n",
      "training [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
      "validation [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58]\n",
      "training [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
      "validation [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78]\n",
      "training [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 99]\n",
      "validation [80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98]\n"
     ]
    }
   ],
   "source": [
    "# Dataset indices\n",
    "dataset_size = 100\n",
    "k_folds = 5\n",
    "\n",
    "# YOUR TASK:\n",
    "# Generate k-fold cross-validation splits:\n",
    "# 1. Divide dataset into k equal folds (or as equal as possible)\n",
    "# 2. For each fold:\n",
    "#    - That fold becomes validation set\n",
    "#    - All other folds become training set\n",
    "# 3. Print train/val splits for each fold\n",
    "# 4. Ensure no data leakage (no overlap between train/val in same fold)\n",
    "#\n",
    "# Expected output:\n",
    "# Fold 1/5:\n",
    "#   Train indices: [20-99] (80 samples)\n",
    "#   Val indices: [0-19] (20 samples)\n",
    "#\n",
    "# Fold 2/5:\n",
    "#   Train indices: [0-19, 40-99] (80 samples)\n",
    "#   Val indices: [20-39] (20 samples)\n",
    "#\n",
    "# Fold 3/5:\n",
    "#   Train indices: [0-39, 60-99] (80 samples)\n",
    "#   Val indices: [40-59] (20 samples)\n",
    "#\n",
    "# ... etc\n",
    "#\n",
    "# Verification:\n",
    "# âœ“ Each sample used for validation exactly once\n",
    "# âœ“ Each sample used for training exactly 4 times\n",
    "# âœ“ No overlap between train/val in any fold\n",
    "\n",
    "# YOUR CODE:\n",
    "fold_size = dataset_size // k_folds\n",
    "all_indices = list(range(dataset_size))\n",
    "for fold  in range(k_folds):\n",
    "    \n",
    "    start=fold*fold_size\n",
    "    end=min(start+fold_size-1,dataset_size)\n",
    "    val_indices=list(range(start,end))\n",
    "    train_indices=[i for i in all_indices if i not in val_indices]\n",
    "    print(f\"training {train_indices}\")\n",
    "    print(f\"validation {val_indices}\")\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d53cad4",
   "metadata": {},
   "source": [
    "DAY 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798c04ba",
   "metadata": {},
   "source": [
    "\n",
    "Exercise 1: Feature Scaling (Min-Max Normalization) (20 min)\n",
    "MLE Context: Normalize features to [0, 1] range before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b65c774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.125, 0.5416666666666666, 0.2708333333333333, 1.0, 0.0, 0.7291666666666666, 0.3958333333333333, 0.20833333333333334, 0.6666666666666666, 0.4583333333333333]\n",
      "[0.046052631578947366, 0.6381578947368421, 0.2565789473684211, 1.0, 0.0, 0.4407894736842105, 0.2894736842105263, 0.1513157894736842, 0.5394736842105263, 0.39473684210526316]\n",
      "[0.16666666666666666, 0.8333333333333334, 0.4583333333333333, 1.0, 0.0, 0.6666666666666666, 0.5416666666666666, 0.2916666666666667, 0.8958333333333334, 0.6041666666666666]\n",
      "sample 0: age=0.12, incomes=0.05 and credit=0.17 not done\n",
      "sample 1: age=0.54, incomes=0.64 and credit=0.83 done\n",
      "sample 2: age=0.27, incomes=0.26 and credit=0.46 not done\n",
      "sample 3: age=1.00, incomes=1.00 and credit=1.00 done\n",
      "sample 4: age=0.00, incomes=0.00 and credit=0.00 not done\n",
      "sample 5: age=0.73, incomes=0.44 and credit=0.67 not done\n",
      "sample 6: age=0.40, incomes=0.29 and credit=0.54 not done\n",
      "sample 7: age=0.21, incomes=0.15 and credit=0.29 not done\n",
      "sample 8: age=0.67, incomes=0.54 and credit=0.90 done\n",
      "sample 9: age=0.46, incomes=0.39 and credit=0.60 not done\n"
     ]
    }
   ],
   "source": [
    "# Raw features (different scales)\n",
    "ages = [25, 45, 32, 67, 19, 54, 38, 29, 51, 41]\n",
    "incomes = [35000, 125000, 67000, 180000, 28000, 95000, 72000, 51000, 110000, 88000]\n",
    "credit_scores = [620, 780, 690, 820, 580, 740, 710, 650, 795, 725]\n",
    "\n",
    "# YOUR TASK:\n",
    "# 1. For each feature, apply min-max normalization:\n",
    "#    normalized = (value - min) / (max - min)\n",
    "# 2. Create normalized versions of all three lists\n",
    "# 3. Verify all values are in [0, 1] range\n",
    "# 4. Find which samples have ALL features > 0.5 (after normalization)\n",
    "#\n",
    "# Expected output:\n",
    "# Ages - min: 19, max: 67, range: 48\n",
    "# Normalized ages: [0.125, 0.542, 0.271, 1.0, 0.0, 0.729, ...]\n",
    "#\n",
    "# Incomes - min: 28000, max: 180000, range: 152000\n",
    "# Normalized incomes: [0.046, 0.638, 0.257, 1.0, 0.0, 0.441, ...]\n",
    "#\n",
    "# Credit scores - min: 580, max: 820, range: 240\n",
    "# Normalized credit_scores: [0.167, 0.833, 0.458, 1.0, 0.0, ...]\n",
    "#\n",
    "# Samples with ALL features > 0.5:\n",
    "#   Sample 1: age=0.542, income=0.638, credit=0.833 âœ“\n",
    "#   Sample 3: age=1.0, income=1.0, credit=1.0 âœ“\n",
    "#   Sample 5: age=0.729, income=0.441, credit=0.667 âœ— (income too low)\n",
    "\n",
    "# YOUR CODE:\n",
    "def normalize(values):\n",
    "    \"\"\"Min-max normalization to [0, 1]\"\"\"\n",
    "    min_val = min(values)\n",
    "    max_val = max(values)\n",
    "    range_val = max_val - min_val\n",
    "    temp_list=[(i-min_val)/ range_val for i in values ]\n",
    "    print(temp_list)\n",
    "    # Return list of normalized values\n",
    "    return temp_list\n",
    "\n",
    "normalized_ages = normalize(ages)\n",
    "normalized_incomes = normalize(incomes)\n",
    "normalized_credit_scores = normalize(credit_scores)\n",
    "\n",
    "# Find samples with all features > 0.5\n",
    "high_quality_samples = []\n",
    "for i in range(len(ages)):\n",
    "    if normalized_ages[i]> 0.5 and normalized_incomes[i]> 0.5 and normalized_credit_scores[i]> 0.5:\n",
    "        print(f\"sample {i}: age={normalized_ages[i]:.2f}, incomes={normalized_incomes[i]:.2f} and credit={normalized_credit_scores[i]:.2f} done\")\n",
    "    else:\n",
    "        print(f\"sample {i}: age={normalized_ages[i]:.2f}, incomes={normalized_incomes[i]:.2f} and credit={normalized_credit_scores[i]:.2f} not done\")\n",
    "\n",
    "    \n",
    "    # Check if all three normalized features > 0.5\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccc7c4a",
   "metadata": {},
   "source": [
    "Exercise 2: Train-Test Split with Stratification (20 min)\n",
    "MLE Context: Split data while preserving class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7868c7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training distribution\n",
      "total samples : 16\n",
      "zeroes in training 8\n",
      "ones in training 8\n"
     ]
    }
   ],
   "source": [
    "# Dataset: features paired with labels\n",
    "X = [  # Features (simplified as single values)\n",
    "    10, 15, 12, 18, 22, 25, 30, 35, 28, 32,\n",
    "    40, 45, 38, 42, 48, 50, 55, 52, 58, 60\n",
    "]\n",
    "\n",
    "y = [  # Labels (binary)\n",
    "    0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
    "    0, 0, 0, 0, 0, 1, 1, 1, 1, 1\n",
    "]\n",
    "# Class distribution: 10 zeros, 10 ones (perfectly balanced)\n",
    "\n",
    "test_size = 0.2  # 20% for testing\n",
    "\n",
    "# YOUR TASK:\n",
    "# 1. Calculate how many samples for test set (20% of 20 = 4)\n",
    "# 2. To preserve class balance, need 2 zeros and 2 ones in test\n",
    "# 3. Manually create stratified split:\n",
    "#    - Find indices of all class 0 samples\n",
    "#    - Find indices of all class 1 samples\n",
    "#    - Take first 2 from each class for test (simple strategy)\n",
    "#    - Remaining goes to train\n",
    "# 4. Verify class distribution is preserved\n",
    "#\n",
    "# Expected output:\n",
    "# Total samples: 20 (10 class 0, 10 class 1)\n",
    "# Test size: 4 samples\n",
    "#\n",
    "# Test set:\n",
    "#   Indices: [0, 1, 5, 6]\n",
    "#   Features: [10, 15, 25, 30]\n",
    "#   Labels: [0, 0, 1, 1]\n",
    "#   Class distribution: 2 zeros (50%), 2 ones (50%) âœ“\n",
    "#\n",
    "# Train set:\n",
    "#   Indices: [2, 3, 4, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
    "#   16 samples\n",
    "#   Class distribution: 8 zeros (50%), 8 ones (50%) âœ“\n",
    "\n",
    "# YOUR CODE:\n",
    "# Find indices for each class\n",
    "class_0_indices = []\n",
    "class_1_indices = []\n",
    "\n",
    "for i, label in enumerate(y):\n",
    "    if label == 0:\n",
    "        class_0_indices.append(i)\n",
    "    else:\n",
    "        class_1_indices.append(i)\n",
    "\n",
    "# Take first 2 from each class for test\n",
    "samples_per_class = 2\n",
    "\n",
    "test_indices = class_0_indices[:samples_per_class] + class_1_indices[:samples_per_class]\n",
    "train_indices = class_0_indices[samples_per_class:] + class_1_indices[samples_per_class:]\n",
    "\n",
    "# Create actual splits\n",
    "X_test = [X[i] for i in test_indices]\n",
    "y_test = [y[i] for i in test_indices]\n",
    "\n",
    "# ... create X_train, y_train\n",
    "X_train=[X[j] for j in train_indices]\n",
    "y_train=[y[j] for j in train_indices]\n",
    "print(f\"Training distribution\")\n",
    "print(f\"total samples : {len(X_train)}\")\n",
    "\n",
    "print(f\"zeroes in training {y_train.count(0)}\")\n",
    "print(f\"ones in training {y_train.count(1)}\")\n",
    "\n",
    "\n",
    "# Verify distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276a38bc",
   "metadata": {},
   "source": [
    "Exercise 3: Moving Average for Smoothing Training Curves (20 min)\n",
    "MLE Context: Smooth noisy training loss curves for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1979ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw training losses (noisy)\n",
    "losses = [\n",
    "    0.95, 0.89, 0.92, 0.85, 0.82, 0.88, 0.79, 0.75,\n",
    "    0.78, 0.71, 0.73, 0.68, 0.70, 0.65, 0.63, 0.66,\n",
    "    0.61, 0.59, 0.62, 0.58, 0.56, 0.57, 0.54, 0.53\n",
    "]\n",
    "\n",
    "window_size = 3  # Moving average window\n",
    "\n",
    "# YOUR TASK:\n",
    "# 1. Calculate moving average with window_size=3\n",
    "#    For each position i, average losses[i-1], losses[i], losses[i+1]\n",
    "#    (For edges, use available values)\n",
    "# 2. Compare original vs smoothed\n",
    "# 3. Calculate how much noise was reduced (variance before/after)\n",
    "#\n",
    "# Expected output:\n",
    "# Original losses (first 10): [0.95, 0.89, 0.92, 0.85, 0.82, ...]\n",
    "# Smoothed losses (first 10): [0.92, 0.92, 0.887, 0.863, 0.850, ...]\n",
    "#\n",
    "# Noise reduction:\n",
    "#   Original variance: 0.0234\n",
    "#   Smoothed variance: 0.0198\n",
    "#   Reduction: 15.4%\n",
    "\n",
    "# YOUR CODE:\n",
    "smoothed_losses = []\n",
    "\n",
    "for i in range(len(losses)):\n",
    "    # Determine window boundaries\n",
    "    start = max(0, i - window_size // 2)\n",
    "    end = min(len(losses), i + window_size // 2 + 1)\n",
    "    \n",
    "    # Calculate average of window\n",
    "    window = losses[start:end]\n",
    "    avg = sum(window) / len(window)\n",
    "    smoothed_losses.append(avg)\n",
    "\n",
    "# Calculate variance (simple version: average of squared differences from mean)\n",
    "def calculate_variance(values):\n",
    "    mean = sum(values) / len(values)\n",
    "    squared_diffs = [(x - mean) ** 2 for x in values]\n",
    "    return sum(squared_diffs) / len(squared_diffs)\n",
    "\n",
    "original_var = calculate_variance(losses)\n",
    "smoothed_var = calculate_variance(smoothed_losses)\n",
    "\n",
    "# Print comparison\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc2861f",
   "metadata": {},
   "source": [
    "DAY 4: Dictionaries & Sets (75 min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbced16",
   "metadata": {},
   "source": [
    "Exercise 1: Hyperparameter Experiment Tracker (25 min)\n",
    "MLE Context: Track and compare multiple experiment runs with different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f2094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment results\n",
    "experiments = [\n",
    "    {\"id\": \"exp_001\", \"lr\": 0.001, \"batch_size\": 32, \"optimizer\": \"adam\", \"val_acc\": 0.847, \"train_time\": 120},\n",
    "    {\"id\": \"exp_002\", \"lr\": 0.01, \"batch_size\": 32, \"optimizer\": \"adam\", \"val_acc\": 0.823, \"train_time\": 118},\n",
    "    {\"id\": \"exp_003\", \"lr\": 0.001, \"batch_size\": 64, \"optimizer\": \"adam\", \"val_acc\": 0.856, \"train_time\": 95},\n",
    "    {\"id\": \"exp_004\", \"lr\": 0.001, \"batch_size\": 32, \"optimizer\": \"sgd\", \"val_acc\": 0.812, \"train_time\": 115},\n",
    "    {\"id\": \"exp_005\", \"lr\": 0.01, \"batch_size\": 64, \"optimizer\": \"adam\", \"val_acc\": 0.834, \"train_time\": 92},\n",
    "    {\"id\": \"exp_006\", \"lr\": 0.001, \"batch_size\": 64, \"optimizer\": \"sgd\", \"val_acc\": 0.829, \"train_time\": 90},\n",
    "]\n",
    "\n",
    "# YOUR TASK:\n",
    "# 1. Group experiments by optimizer\n",
    "# 2. For each optimizer, calculate average validation accuracy\n",
    "# 3. Find best experiment overall (highest val_acc)\n",
    "# 4. Find best experiment for each optimizer\n",
    "# 5. Analyze: Does larger batch_size always train faster?\n",
    "#\n",
    "# Expected output:\n",
    "# Experiments by optimizer:\n",
    "#   adam: 4 experiments, avg val_acc: 0.840\n",
    "#   sgd: 2 experiments, avg val_acc: 0.8205\n",
    "#\n",
    "# Best overall: exp_003 (val_acc: 0.856, lr=0.001, batch_size=64, optimizer=adam)\n",
    "# Best adam: exp_003 (val_acc: 0.856)\n",
    "# Best sgd: exp_006 (val_acc: 0.829)\n",
    "#\n",
    "# Batch size analysis:\n",
    "#   batch_size=32: avg train_time=117.67s\n",
    "#   batch_size=64: avg train_time=92.33s\n",
    "#   Conclusion: Larger batch size IS faster âœ“\n",
    "\n",
    "# YOUR CODE:\n",
    "# Group by optimizer\n",
    "optimizer_groups = {}\n",
    "\n",
    "for exp in experiments:\n",
    "    opt = exp[\"optimizer\"]\n",
    "    if opt not in optimizer_groups:\n",
    "        optimizer_groups[opt] = []\n",
    "    optimizer_groups[opt].append(exp)\n",
    "\n",
    "# Calculate averages per optimizer\n",
    "# Find best experiments\n",
    "# Analyze batch size impact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63adb2f2",
   "metadata": {},
   "source": [
    "Exercise 2: Feature Importance Consensus (25 min)\n",
    "MLE Context: Multiple models rank features differently. Find consensus features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d3870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three different model runs, each gives top features with importance scores\n",
    "model1_features = {\n",
    "    \"age\": 0.25,\n",
    "    \"income\": 0.22,\n",
    "    \"credit_score\": 0.18,\n",
    "    \"employment_years\": 0.15,\n",
    "    \"loan_amount\": 0.12,\n",
    "    \"debt_ratio\": 0.08\n",
    "}\n",
    "\n",
    "model2_features = {\n",
    "    \"income\": 0.28,\n",
    "    \"credit_score\": 0.24,\n",
    "    \"age\": 0.19,\n",
    "    \"loan_amount\": 0.14,\n",
    "    \"num_accounts\": 0.10,\n",
    "    \"debt_ratio\": 0.05\n",
    "}\n",
    "\n",
    "model3_features = {\n",
    "    \"credit_score\": 0.30,\n",
    "    \"income\": 0.25,\n",
    "    \"employment_years\": 0.18,\n",
    "    \"age\": 0.12,\n",
    "    \"loan_amount\": 0.10,\n",
    "    \"payment_history\": 0.05\n",
    "}\n",
    "\n",
    "# YOUR TASK:\n",
    "# 1. Find features that appear in ALL models (intersection)\n",
    "# 2. Find features that appear in ANY model (union)\n",
    "# 3. For common features, calculate average importance across models\n",
    "# 4. Rank all features by average importance (handle features not in all models)\n",
    "# 5. Identify \"stable\" features (appear in all models with importance > 0.15)\n",
    "#\n",
    "# Expected output:\n",
    "# Features in ALL models: {'age', 'income', 'credit_score', 'loan_amount', 'debt_ratio'}\n",
    "# Features in ANY model: {'age', 'income', 'credit_score', 'employment_years', \n",
    "#                          'loan_amount', 'debt_ratio', 'num_accounts', 'payment_history'}\n",
    "#\n",
    "# Average importance (for features in all models):\n",
    "#   credit_score: 0.24 (model1: 0.18, model2: 0.24, model3: 0.30)\n",
    "#   income: 0.25 (model1: 0.22, model2: 0.28, model3: 0.25)\n",
    "#   age: 0.187 (model1: 0.25, model2: 0.19, model3: 0.12)\n",
    "#   loan_amount: 0.12 (model1: 0.12, model2: 0.14, model3: 0.10)\n",
    "#   debt_ratio: 0.067 (model1: 0.08, model2: 0.05, model3: N/A... wait, need to handle)\n",
    "#\n",
    "# Stable features (in all models, avg > 0.15): income, credit_score, age\n",
    "# Recommendation: Focus on these 3 features for model simplification\n",
    "\n",
    "# YOUR CODE:\n",
    "# Get all feature sets\n",
    "features1 = set(model1_features.keys())\n",
    "features2 = set(model2_features.keys())\n",
    "features3 = set(model3_features.keys())\n",
    "\n",
    "# Intersection and union\n",
    "common_features = features1 & features2 & features3\n",
    "all_features = features1 | features2 | features3\n",
    "\n",
    "# Calculate average importance\n",
    "feature_avg_importance = {}\n",
    "\n",
    "for feature in all_features:\n",
    "    importances = []\n",
    "    if feature in model1_features:\n",
    "        importances.append(model1_features[feature])\n",
    "    if feature in model2_features:\n",
    "        importances.append(model2_features[feature])\n",
    "    if feature in model3_features:\n",
    "        importances.append(model3_features[feature])\n",
    "    \n",
    "    avg = sum(importances) / len(importances)\n",
    "    feature_avg_importance[feature] = avg\n",
    "\n",
    "# Sort by importance\n",
    "# Identify stable features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2a663d",
   "metadata": {},
   "source": [
    "Exercise 3: Dataset Deduplication with Hash-Based Detection (25 min)\n",
    "MLE Context: Remove duplicate samples from training data (critical preprocessing step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2fb057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training samples (some are duplicates)\n",
    "samples = [\n",
    "    {\"text\": \"machine learning is great\", \"label\": 1, \"source\": \"tweet\"},\n",
    "    {\"text\": \"python programming\", \"label\": 0, \"source\": \"article\"},\n",
    "    {\"text\": \"machine learning is great\", \"label\": 1, \"source\": \"blog\"},  # Duplicate text, different source\n",
    "    {\"text\": \"deep learning basics\", \"label\": 1, \"source\": \"tutorial\"},\n",
    "    {\"text\": \"python programming\", \"label\": 0, \"source\": \"article\"},  # Exact duplicate\n",
    "    {\"text\": \"data science tips\", \"label\": 0, \"source\": \"blog\"},\n",
    "    {\"text\": \"machine learning is great\", \"label\": 0, \"source\": \"tweet\"},  # Same text, DIFFERENT label! âš ï¸\n",
    "    {\"text\": \"deep learning basics\", \"label\": 1, \"source\": \"course\"},  # Duplicate text\n",
    "]\n",
    "\n",
    "# YOUR TASK:\n",
    "# 1. Remove exact duplicates (all fields identical)\n",
    "# 2. Identify duplicate texts with SAME label (keep first occurrence)\n",
    "# 3. Identify duplicate texts with DIFFERENT labels (critical data quality issue!)\n",
    "# 4. Track: text -> set of labels seen\n",
    "# 5. Generate deduplication report\n",
    "#\n",
    "# Expected output:\n",
    "# Original dataset: 8 samples\n",
    "#\n",
    "# Deduplication analysis:\n",
    "#   Exact duplicates found: 1 (sample 4 matches sample 1)\n",
    "#   Text duplicates (same label): 2 \n",
    "#     - \"machine learning is great\" (label 1): appears 2 times\n",
    "#     - \"deep learning basics\" (label 1): appears 2 times\n",
    "#   \n",
    "#   âš ï¸ CRITICAL: Text with conflicting labels: 1\n",
    "#     - \"machine learning is great\": labels {0, 1}\n",
    "#       Sample 0: label=1, source=tweet\n",
    "#       Sample 2: label=1, source=blog\n",
    "#       Sample 6: label=0, source=tweet âš ï¸\n",
    "#\n",
    "# After deduplication: 5 unique samples\n",
    "# Samples removed: 3\n",
    "# Samples requiring manual review: 1 (conflicting labels)\n",
    "#\n",
    "# Clean dataset:\n",
    "#   {\"text\": \"machine learning is great\", \"label\": 1, \"source\": \"tweet\"}\n",
    "#   {\"text\": \"python programming\", \"label\": 0, \"source\": \"article\"}\n",
    "#   {\"text\": \"deep learning basics\", \"label\": 1, \"source\": \"tutorial\"}\n",
    "#   {\"text\": \"data science tips\", \"label\": 0, \"source\": \"blog\"}\n",
    "#   Note: Excluded \"machine learning is great\" with label=0 due to conflict\n",
    "\n",
    "# YOUR CODE:\n",
    "# Track what we've seen\n",
    "seen_exact = set()  # For exact duplicates\n",
    "text_to_labels = {}  # Track all labels per text\n",
    "text_to_samples = {}  # Track all samples per text\n",
    "unique_samples = []\n",
    "exact_duplicates = []\n",
    "conflicting_samples = []\n",
    "\n",
    "for idx, sample in enumerate(samples):\n",
    "    # Create hashable representation for exact duplicate detection\n",
    "    # Hint: Convert dict to sorted tuple of items\n",
    "    exact_key = tuple(sorted(sample.items()))\n",
    "    \n",
    "    # Check for exact duplicate\n",
    "    if exact_key in seen_exact:\n",
    "        exact_duplicates.append(idx)\n",
    "        continue\n",
    "    \n",
    "    seen_exact.add(exact_key)\n",
    "    \n",
    "    # Track text and labels\n",
    "    text = sample[\"text\"]\n",
    "    label = sample[\"label\"]\n",
    "    \n",
    "    if text not in text_to_labels:\n",
    "        text_to_labels[text] = set()\n",
    "        text_to_samples[text] = []\n",
    "    \n",
    "    text_to_labels[text].add(label)\n",
    "    text_to_samples[text].append((idx, sample))\n",
    "\n",
    "# Now process: keep first occurrence of each text, but flag conflicts\n",
    "for text, label_set in text_to_labels.items():\n",
    "    samples_for_text = text_to_samples[text]\n",
    "    \n",
    "    if len(label_set) > 1:\n",
    "        # Conflicting labels!\n",
    "        conflicting_samples.append({\n",
    "            \"text\": text,\n",
    "            \"labels\": label_set,\n",
    "            \"samples\": samples_for_text\n",
    "        })\n",
    "        # Skip this text entirely (needs manual review)\n",
    "    else:\n",
    "        # Keep first occurrence\n",
    "        unique_samples.append(samples_for_text[0][1])\n",
    "\n",
    "# Print detailed report\n",
    "print(f\"Original dataset: {len(samples)} samples\\n\")\n",
    "print(\"Deduplication analysis:\")\n",
    "print(f\"  Exact duplicates found: {len(exact_duplicates)}\")\n",
    "\n",
    "# Count text duplicates with same label\n",
    "text_dup_count = sum(1 for text, samples in text_to_samples.items() \n",
    "                     if len(samples) > 1 and len(text_to_labels[text]) == 1)\n",
    "print(f\"  Text duplicates (same label): {text_dup_count}\")\n",
    "\n",
    "# Report conflicts\n",
    "if conflicting_samples:\n",
    "    print(f\"\\n  âš ï¸ CRITICAL: Texts with conflicting labels: {len(conflicting_samples)}\")\n",
    "    for conflict in conflicting_samples:\n",
    "        print(f\"    - \\\"{conflict['text']}\\\": labels {conflict['labels']}\")\n",
    "        for idx, sample in conflict['samples']:\n",
    "            print(f\"      Sample {idx}: label={sample['label']}, source={sample['source']}\")\n",
    "\n",
    "print(f\"\\nAfter deduplication: {len(unique_samples)} unique samples\")\n",
    "print(f\"Samples removed: {len(samples) - len(unique_samples)}\")\n",
    "print(f\"Samples requiring manual review: {len(conflicting_samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83216868",
   "metadata": {},
   "source": [
    "DAY 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6876f359",
   "metadata": {},
   "source": [
    "Exercise 1: Comprehensive Metrics Library (30 min)\n",
    "MLE Context: Build reusable evaluation functions that work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad15680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TASK: Implement these functions for binary classification metrics\n",
    "\n",
    "def calculate_confusion_matrix(predictions, labels):\n",
    "    \"\"\"\n",
    "    Calculate TP, TN, FP, FN from predictions and labels.\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of binary predictions [0, 1, 1, ...]\n",
    "        labels: List of true labels [0, 1, 0, ...]\n",
    "    \n",
    "    Returns:\n",
    "        Dict: {\"tp\": int, \"tn\": int, \"fp\": int, \"fn\": int}\n",
    "    \n",
    "    Example:\n",
    "        >>> calculate_confusion_matrix([1, 0, 1, 0], [1, 0, 0, 0])\n",
    "        {\"tp\": 1, \"tn\": 2, \"fp\": 1, \"fn\": 0}\n",
    "    \"\"\"\n",
    "    tp = tn = fp = fn = 0\n",
    "    \n",
    "    for pred, label in zip(predictions, labels):\n",
    "        # Your logic here\n",
    "        pass\n",
    "    \n",
    "    return {\"tp\": tp, \"tn\": tn, \"fp\": fp, \"fn\": fn}\n",
    "\n",
    "\n",
    "def calculate_accuracy(predictions, labels):\n",
    "    \"\"\"\n",
    "    Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    \n",
    "    Returns:\n",
    "        Float: Accuracy between 0 and 1\n",
    "    \"\"\"\n",
    "    cm = calculate_confusion_matrix(predictions, labels)\n",
    "    # Use confusion matrix to calculate accuracy\n",
    "    pass\n",
    "\n",
    "\n",
    "def calculate_precision(predictions, labels):\n",
    "    \"\"\"\n",
    "    Precision = TP / (TP + FP)\n",
    "    Measures: Of all positive predictions, how many were correct?\n",
    "    \n",
    "    Returns:\n",
    "        Float: Precision, or None if no positive predictions\n",
    "    \"\"\"\n",
    "    cm = calculate_confusion_matrix(predictions, labels)\n",
    "    \n",
    "    if cm[\"tp\"] + cm[\"fp\"] == 0:\n",
    "        return None  # No positive predictions\n",
    "    \n",
    "    # Calculate precision\n",
    "    pass\n",
    "\n",
    "\n",
    "def calculate_recall(predictions, labels):\n",
    "    \"\"\"\n",
    "    Recall = TP / (TP + FN)\n",
    "    Measures: Of all actual positives, how many did we find?\n",
    "    \n",
    "    Returns:\n",
    "        Float: Recall, or None if no actual positives\n",
    "    \"\"\"\n",
    "    # Your code\n",
    "    pass\n",
    "\n",
    "\n",
    "def calculate_f1_score(predictions, labels):\n",
    "    \"\"\"\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    Harmonic mean of precision and recall.\n",
    "    \n",
    "    Returns:\n",
    "        Float: F1 score, or None if can't calculate\n",
    "    \"\"\"\n",
    "    precision = calculate_precision(predictions, labels)\n",
    "    recall = calculate_recall(predictions, labels)\n",
    "    \n",
    "    if precision is None or recall is None:\n",
    "        return None\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate F1\n",
    "    pass\n",
    "\n",
    "\n",
    "def calculate_specificity(predictions, labels):\n",
    "    \"\"\"\n",
    "    Specificity = TN / (TN + FP)\n",
    "    Measures: Of all actual negatives, how many did we correctly identify?\n",
    "    \n",
    "    Returns:\n",
    "        Float: Specificity\n",
    "    \"\"\"\n",
    "    # Your code\n",
    "    pass\n",
    "\n",
    "\n",
    "def evaluate_model_comprehensive(predictions, labels):\n",
    "    \"\"\"\n",
    "    Calculate ALL metrics and return organized report.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with structure:\n",
    "        {\n",
    "            \"confusion_matrix\": {...},\n",
    "            \"primary_metrics\": {\"accuracy\": float, \"f1\": float},\n",
    "            \"precision_recall\": {\"precision\": float, \"recall\": float},\n",
    "            \"specificity\": float,\n",
    "            \"class_distribution\": {\"predicted_positive\": int, \"actual_positive\": int}\n",
    "        }\n",
    "    \"\"\"\n",
    "    cm = calculate_confusion_matrix(predictions, labels)\n",
    "    \n",
    "    report = {\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"primary_metrics\": {\n",
    "            \"accuracy\": calculate_accuracy(predictions, labels),\n",
    "            \"f1\": calculate_f1_score(predictions, labels)\n",
    "        },\n",
    "        \"precision_recall\": {\n",
    "            \"precision\": calculate_precision(predictions, labels),\n",
    "            \"recall\": calculate_recall(predictions, labels)\n",
    "        },\n",
    "        \"specificity\": calculate_specificity(predictions, labels),\n",
    "        \"class_distribution\": {\n",
    "            \"predicted_positive\": sum(predictions),\n",
    "            \"actual_positive\": sum(labels),\n",
    "            \"predicted_negative\": len(predictions) - sum(predictions),\n",
    "            \"actual_negative\": len(labels) - sum(labels)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return report\n",
    "\n",
    "\n",
    "# TEST YOUR FUNCTIONS:\n",
    "test_predictions = [1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1]\n",
    "test_labels =      [1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1]\n",
    "\n",
    "report = evaluate_model_comprehensive(test_predictions, test_labels)\n",
    "\n",
    "print(\"=== Model Evaluation Report ===\")\n",
    "print(f\"Confusion Matrix: {report['confusion_matrix']}\")\n",
    "print(f\"Accuracy: {report['primary_metrics']['accuracy']:.3f}\")\n",
    "print(f\"F1 Score: {report['primary_metrics']['f1']:.3f}\")\n",
    "print(f\"Precision: {report['precision_recall']['precision']:.3f}\")\n",
    "print(f\"Recall: {report['precision_recall']['recall']:.3f}\")\n",
    "print(f\"Specificity: {report['specificity']:.3f}\")\n",
    "print(f\"Class Distribution: {report['class_distribution']}\")\n",
    "\n",
    "# Expected output (approximately):\n",
    "# Confusion Matrix: {'tp': 5, 'tn': 3, 'fp': 2, 'fn': 2}\n",
    "# Accuracy: 0.667\n",
    "# F1 Score: 0.714\n",
    "# Precision: 0.714\n",
    "# Recall: 0.714\n",
    "# Specificity: 0.600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae3f655",
   "metadata": {},
   "source": [
    "Exercise 2: Data Preprocessing Pipeline (25 min)\n",
    "MLE Context: Build modular cleaning functions for production pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text: lowercase, strip whitespace, remove extra spaces.\n",
    "    \n",
    "    Args:\n",
    "        text: String or None\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned string, or None if invalid input\n",
    "    \n",
    "    Example:\n",
    "        >>> clean_text(\"  HELLO   World  \")\n",
    "        \"hello world\"\n",
    "    \"\"\"\n",
    "    if text is None or not isinstance(text, str):\n",
    "        return None\n",
    "    \n",
    "    # Clean and return\n",
    "    # Hint: strip(), lower(), and handle multiple spaces\n",
    "    pass\n",
    "\n",
    "\n",
    "def parse_numeric(value, value_type=\"int\"):\n",
    "    \"\"\"\n",
    "    Parse string to numeric value with validation.\n",
    "    \n",
    "    Args:\n",
    "        value: String, int, or float\n",
    "        value_type: \"int\" or \"float\"\n",
    "    \n",
    "    Returns:\n",
    "        Parsed number, or None if invalid\n",
    "    \n",
    "    Example:\n",
    "        >>> parse_numeric(\"123\", \"int\")\n",
    "        123\n",
    "        >>> parse_numeric(\"12.5\", \"float\")\n",
    "        12.5\n",
    "        >>> parse_numeric(\"abc\", \"int\")\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Handle already numeric\n",
    "    if isinstance(value, (int, float)):\n",
    "        if value_type == \"int\":\n",
    "            return int(value)\n",
    "        return float(value)\n",
    "    \n",
    "    # Try to parse string\n",
    "    # Handle exceptions\n",
    "    pass\n",
    "\n",
    "\n",
    "def validate_range(value, min_val, max_val):\n",
    "    \"\"\"\n",
    "    Check if value is within valid range.\n",
    "    \n",
    "    Returns:\n",
    "        value if valid, None if out of range\n",
    "    \n",
    "    Example:\n",
    "        >>> validate_range(25, 0, 100)\n",
    "        25\n",
    "        >>> validate_range(-5, 0, 100)\n",
    "        None\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def extract_price(price_string):\n",
    "    \"\"\"\n",
    "    Extract numeric price from string.\n",
    "    Handle: \"$50.00\", \"50\", \"$1,234.56\", \"â‚¬45.99\"\n",
    "    \n",
    "    Returns:\n",
    "        Float price, or None if can't parse\n",
    "    \n",
    "    Example:\n",
    "        >>> extract_price(\"$1,234.56\")\n",
    "        1234.56\n",
    "        >>> extract_price(\"â‚¬45.99\")\n",
    "        45.99\n",
    "    \"\"\"\n",
    "    if not isinstance(price_string, str):\n",
    "        return parse_numeric(price_string, \"float\")\n",
    "    \n",
    "    # Remove currency symbols and commas\n",
    "    cleaned = price_string\n",
    "    for char in ['$', 'â‚¬', 'Â£', ',']:\n",
    "        cleaned = cleaned.replace(char, '')\n",
    "    \n",
    "    # Parse as float\n",
    "    pass\n",
    "\n",
    "\n",
    "def clean_and_validate_record(record, schema):\n",
    "    \"\"\"\n",
    "    Clean entire record based on schema definition.\n",
    "    \n",
    "    Args:\n",
    "        record: Dict with raw data\n",
    "        schema: Dict defining field types and validation\n",
    "            Example: {\n",
    "                \"name\": {\"type\": \"text\", \"required\": True},\n",
    "                \"age\": {\"type\": \"int\", \"min\": 0, \"max\": 120},\n",
    "                \"price\": {\"type\": \"price\", \"min\": 0}\n",
    "            }\n",
    "    \n",
    "    Returns:\n",
    "        Tuple: (cleaned_record, errors)\n",
    "        - cleaned_record: Dict with cleaned data, or None if critical errors\n",
    "        - errors: List of error messages\n",
    "    \n",
    "    Example:\n",
    "        >>> schema = {\n",
    "        ...     \"name\": {\"type\": \"text\", \"required\": True},\n",
    "        ...     \"age\": {\"type\": \"int\", \"min\": 0, \"max\": 120}\n",
    "        ... }\n",
    "        >>> clean_and_validate_record(\n",
    "        ...     {\"name\": \"  Alice  \", \"age\": \"25\"},\n",
    "        ...     schema\n",
    "        ... )\n",
    "        ({\"name\": \"alice\", \"age\": 25}, [])\n",
    "    \"\"\"\n",
    "    cleaned = {}\n",
    "    errors = []\n",
    "    \n",
    "    for field_name, field_schema in schema.items():\n",
    "        field_type = field_schema.get(\"type\")\n",
    "        required = field_schema.get(\"required\", False)\n",
    "        \n",
    "        # Get raw value\n",
    "        raw_value = record.get(field_name)\n",
    "        \n",
    "        # Check if required field is missing\n",
    "        if required and (raw_value is None or raw_value == \"\"):\n",
    "            errors.append(f\"{field_name}: required field missing\")\n",
    "            continue\n",
    "        \n",
    "        # Clean based on type\n",
    "        if field_type == \"text\":\n",
    "            cleaned_value = clean_text(raw_value)\n",
    "        elif field_type == \"int\":\n",
    "            cleaned_value = parse_numeric(raw_value, \"int\")\n",
    "            if cleaned_value is not None:\n",
    "                # Validate range if specified\n",
    "                min_val = field_schema.get(\"min\")\n",
    "                max_val = field_schema.get(\"max\")\n",
    "                if min_val is not None or max_val is not None:\n",
    "                    cleaned_value = validate_range(\n",
    "                        cleaned_value, \n",
    "                        min_val if min_val is not None else float('-inf'),\n",
    "                        max_val if max_val is not None else float('inf')\n",
    "                    )\n",
    "        elif field_type == \"float\":\n",
    "            cleaned_value = parse_numeric(raw_value, \"float\")\n",
    "        elif field_type == \"price\":\n",
    "            cleaned_value = extract_price(raw_value)\n",
    "            if cleaned_value is not None:\n",
    "                min_val = field_schema.get(\"min\", 0)\n",
    "                if cleaned_value < min_val:\n",
    "                    cleaned_value = None\n",
    "        else:\n",
    "            cleaned_value = raw_value\n",
    "        \n",
    "        # Track errors\n",
    "        if cleaned_value is None and raw_value is not None:\n",
    "            errors.append(f\"{field_name}: invalid value '{raw_value}'\")\n",
    "        else:\n",
    "            cleaned[field_name] = cleaned_value\n",
    "    \n",
    "    # Return None if critical errors (missing required fields)\n",
    "    if any(\"required field missing\" in e for e in errors):\n",
    "        return None, errors\n",
    "    \n",
    "    return cleaned, errors\n",
    "\n",
    "\n",
    "# TEST DATA:\n",
    "schema = {\n",
    "    \"name\": {\"type\": \"text\", \"required\": True},\n",
    "    \"age\": {\"type\": \"int\", \"min\": 18, \"max\": 100},\n",
    "    \"income\": {\"type\": \"price\", \"min\": 0},\n",
    "    \"email\": {\"type\": \"text\", \"required\": True}\n",
    "}\n",
    "\n",
    "test_records = [\n",
    "    {\"name\": \"  Alice Smith  \", \"age\": \"25\", \"income\": \"$50,000\", \"email\": \"alice@example.com\"},\n",
    "    {\"name\": \"Bob\", \"age\": \"17\", \"income\": \"$60,000\", \"email\": \"bob@example.com\"},  # Age too young\n",
    "    {\"name\": \"Charlie\", \"age\": \"invalid\", \"income\": \"$70k\", \"email\": \"charlie@example.com\"},  # Invalid age\n",
    "    {\"name\": \"\", \"age\": \"30\", \"income\": \"$80,000\", \"email\": \"\"},  # Missing required fields\n",
    "]\n",
    "\n",
    "cleaned_records = []\n",
    "failed_records = []\n",
    "\n",
    "for idx, record in enumerate(test_records):\n",
    "    cleaned, errors = clean_and_validate_record(record, schema)\n",
    "    \n",
    "    if cleaned:\n",
    "        cleaned_records.append(cleaned)\n",
    "        if errors:\n",
    "            print(f\"Record {idx}: Cleaned with warnings: {errors}\")\n",
    "    else:\n",
    "        failed_records.append((record, errors))\n",
    "        print(f\"Record {idx}: Failed - {errors}\")\n",
    "\n",
    "print(f\"\\nSuccessfully cleaned: {len(cleaned_records)}/{len(test_records)} records\")\n",
    "print(f\"Failed: {len(failed_records)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ee4e12",
   "metadata": {},
   "source": [
    "Exercise 3: Feature Engineering Function Suite (20 min)\n",
    "MLE Context: Create reusable feature extraction functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9929eef",
   "metadata": {},
   "source": [
    "def extract_text_features(text):\n",
    "    \"\"\"\n",
    "    Extract features from text for ML model.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with keys:\n",
    "        - word_count: Number of words\n",
    "        - char_count: Number of characters (excluding spaces)\n",
    "        - avg_word_length: Average word length\n",
    "        - uppercase_ratio: Ratio of uppercase letters\n",
    "        - digit_count: Number of digits\n",
    "        - special_char_count: Number of special characters (!?.,;:)\n",
    "    \n",
    "    Example:\n",
    "        >>> extract_text_features(\"Hello World! 123\")\n",
    "        {\n",
    "            \"word_count\": 3,\n",
    "            \"char_count\": 13,\n",
    "            \"avg_word_length\": 4.33,\n",
    "            \"uppercase_ratio\": 0.154,\n",
    "            \"digit_count\": 3,\n",
    "            \"special_char_count\": 1\n",
    "        }\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Your code here\n",
    "    # Hint: text.split() for words, iterate chars for counting\n",
    "    pass\n",
    "\n",
    "\n",
    "def create_time_features(timestamp_hour, day_of_week):\n",
    "    \"\"\"\n",
    "    Create time-based categorical features.\n",
    "    \n",
    "    Args:\n",
    "        timestamp_hour: Hour of day (0-23)\n",
    "        day_of_week: Day (0=Monday, 6=Sunday)\n",
    "    \n",
    "    Returns:\n",
    "        Dict with keys:\n",
    "        - is_weekend: Boolean\n",
    "        - is_business_hours: Boolean (9-17, Mon-Fri)\n",
    "        - time_of_day: \"night\"(0-6), \"morning\"(6-12), \"afternoon\"(12-18), \"evening\"(18-24)\n",
    "        - is_peak_hours: Boolean (8-10, 17-19 on weekdays)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def normalize_features(value, min_val, max_val):\n",
    "    \"\"\"\n",
    "    Min-max normalization to [0, 1].\n",
    "    \n",
    "    Returns:\n",
    "        Float between 0 and 1, or None if invalid range\n",
    "    \"\"\"\n",
    "    if max_val == min_val:\n",
    "        return None  # Cannot normalize\n",
    "    \n",
    "    # Formula: (value - min) / (max - min)\n",
    "    pass\n",
    "\n",
    "\n",
    "def create_interaction_features(feature1, feature2):\n",
    "    \"\"\"\n",
    "    Create interaction features between two numeric features.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with keys:\n",
    "        - product: feature1 * feature2\n",
    "        - ratio: feature1 / feature2 (or None if feature2 is 0)\n",
    "        - difference: feature1 - feature2\n",
    "        - sum: feature1 + feature2\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def engineer_features(raw_data, feature_config):\n",
    "    \"\"\"\n",
    "    Apply all feature engineering based on config.\n",
    "    \n",
    "    Args:\n",
    "        raw_data: Dict with raw fields\n",
    "        feature_config: Dict specifying which features to create\n",
    "            Example: {\n",
    "                \"text_features\": {\"field\": \"description\"},\n",
    "                \"time_features\": {\"hour\": \"timestamp_hour\", \"day\": \"day_of_week\"},\n",
    "                \"normalize\": [\n",
    "                    {\"field\": \"price\", \"min\": 0, \"max\": 1000}\n",
    "                ],\n",
    "                \"interactions\": [\n",
    "                    {\"f1\": \"age\", \"f2\": \"income\"}\n",
    "                ]\n",
    "            }\n",
    "    \n",
    "    Returns:\n",
    "        Dict with all engineered features\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Extract text features if configured\n",
    "    if \"text_features\" in feature_config:\n",
    "        text_field = feature_config[\"text_features\"][\"field\"]\n",
    "        text = raw_data.get(text_field)\n",
    "        if text:\n",
    "            text_feats = extract_text_features(text)\n",
    "            if text_feats:\n",
    "                features.update(text_feats)\n",
    "    \n",
    "    # Create time features if configured\n",
    "    if \"time_features\" in feature_config:\n",
    "        hour = raw_data.get(feature_config[\"time_features\"][\"hour\"])\n",
    "        day = raw_data.get(feature_config[\"time_features\"][\"day\"])\n",
    "        if hour is not None and day is not None:\n",
    "            time_feats = create_time_features(hour, day)\n",
    "            features.update(time_feats)\n",
    "    \n",
    "    # Normalize features if configured\n",
    "    # Create interactions if configured\n",
    "    # ... your code\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "# TEST:\n",
    "raw_sample = {\n",
    "    \"description\": \"Great product! Very satisfied with purchase.\",\n",
    "    \"timestamp_hour\": 14,\n",
    "    \"day_of_week\": 2,  # Wednesday\n",
    "    \"price\": 199.99,\n",
    "    \"age\": 35,\n",
    "    \"income\": 75000\n",
    "}\n",
    "\n",
    "config = {\n",
    "    \"text_features\": {\"field\": \"description\"},\n",
    "    \"time_features\": {\"hour\": \"timestamp_hour\", \"day\": \"day_of_week\"},\n",
    "    \"normalize\": [\n",
    "        {\"field\": \"price\", \"min\": 0, \"max\": 500}\n",
    "    ],\n",
    "    \"interactions\": [\n",
    "        {\"f1\": \"age\", \"f2\": \"income\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "features = engineer_features(raw_sample, config)\n",
    "print(\"Engineered features:\")\n",
    "for key, value in features.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Expected features include:\n",
    "# word_count, char_count, avg_word_length, uppercase_ratio, etc.\n",
    "# is_weekend, is_business_hours, time_of_day, is_peak_hours\n",
    "# normalized_price\n",
    "# age_income_product, age_income_ratio, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ca7ca",
   "metadata": {},
   "source": [
    "WEEK 2: INTERMEDIATE PYTHON (Days 6-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297f5f75",
   "metadata": {},
   "source": [
    "DAY 6: List Comprehensions & Lambda Functions (75 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6539a8ff",
   "metadata": {},
   "source": [
    "Exercise 1: Vectorized Feature Transformations (20 min)\n",
    "MLE Context: Apply transformations to entire feature columns efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e15730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature data (each list is a column)\n",
    "ages = [25, 32, 45, 28, 51, 38, 42, 29, 36, 48]\n",
    "incomes = [45000, 72000, 95000, 51000, 110000, 68000, 88000, 55000, 76000, 102000]\n",
    "scores = [0.65, 0.78, 0.85, 0.72, 0.91, 0.68, 0.82, 0.70, 0.79, 0.88]\n",
    "\n",
    "# YOUR TASK: Use list comprehensions for all transformations\n",
    "\n",
    "# 1. Normalize ages to [0, 1]\n",
    "min_age = min(ages)\n",
    "max_age = max(ages)\n",
    "normalized_ages = [  # YOUR CODE HERE using comprehension\n",
    "]\n",
    "\n",
    "# 2. Convert incomes to \"income brackets\": \n",
    "#    \"low\" (<60k), \"medium\" (60-90k), \"high\" (>90k)\n",
    "income_brackets = [  # YOUR CODE HERE\n",
    "]\n",
    "\n",
    "# 3. Create binary target: score >= 0.75 -> 1, else -> 0\n",
    "binary_target = [  # YOUR CODE HERE\n",
    "]\n",
    "\n",
    "# 4. Find indices where BOTH income is \"high\" AND score >= 0.80\n",
    "high_performers_idx = [  # YOUR CODE HERE\n",
    "    # Hint: use enumerate and multiple conditions\n",
    "]\n",
    "\n",
    "# 5. Create combined feature: (normalized_age * score) for all samples\n",
    "interaction_feature = [  # YOUR CODE HERE\n",
    "]\n",
    "\n",
    "# 6. Filter: Get ages where corresponding score > 0.75\n",
    "high_score_ages = [  # YOUR CODE HERE\n",
    "    # Hint: Use zip to iterate both lists together\n",
    "]\n",
    "\n",
    "# Print results\n",
    "print(f\"Normalized ages: {normalized_ages}\")\n",
    "print(f\"Income brackets: {income_brackets}\")\n",
    "print(f\"Binary targets: {binary_target}\")\n",
    "print(f\"High performers (indices): {high_performers_idx}\")\n",
    "print(f\"Interaction features: {interaction_feature[:5]}...\")  # First 5\n",
    "print(f\"Ages with high scores: {high_score_ages}\")\n",
    "\n",
    "# Expected outputs:\n",
    "# Normalized ages: [0.0, 0.269, 0.769, ...]\n",
    "# Income brackets: ['low', 'medium', 'high', 'low', 'high', ...]\n",
    "# Binary targets: [0, 1, 1, 0, 1, ...]\n",
    "# High performers (indices): [2, 4, 6, 9]  # Samples with income='high' and score>=0.80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8077f0",
   "metadata": {},
   "source": [
    "Exercise 2: Lambda Functions for Dynamic Sorting (25 min)\n",
    "MLE Context: Sort model results by different criteria using lambda functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50aef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation results\n",
    "models = [\n",
    "    {\"name\": \"baseline\", \"accuracy\": 0.75, \"f1\": 0.72, \"train_time\": 45, \"model_size_mb\": 12},\n",
    "    {\"name\": \"model_v1\", \"accuracy\": 0.82, \"f1\": 0.80, \"train_time\": 120, \"model_size_mb\": 85},\n",
    "    {\"name\": \"model_v2\", \"accuracy\": 0.85, \"f1\": 0.83, \"train_time\": 180, \"model_size_mb\": 150},\n",
    "    {\"name\": \"model_v3\", \"accuracy\": 0.88, \"f1\": 0.86, \"train_time\": 240, \"model_size_mb\": 220},\n",
    "    {\"name\": \"efficient\", \"accuracy\": 0.80, \"f1\": 0.78, \"train_time\": 60, \"model_size_mb\": 25},\n",
    "]\n",
    "\n",
    "# YOUR TASK: Use lambda functions with sorted()\n",
    "\n",
    "# 1. Sort by accuracy (descending)\n",
    "by_accuracy = sorted(models, key=lambda m: m[\"accuracy\"], reverse=True)\n",
    "\n",
    "# 2. Sort by F1 score (descending)\n",
    "by_f1 = # YOUR CODE\n",
    "\n",
    "# 3. Sort by train_time (ascending - fastest first)\n",
    "by_speed = # YOUR CODE\n",
    "\n",
    "# 4. Sort by model size (ascending - smallest first)\n",
    "by_size = # YOUR CODE\n",
    "\n",
    "# 5. Complex sort: Best \"efficiency score\" \n",
    "#    Efficiency = accuracy / (train_time_minutes * model_size_gb)\n",
    "#    Higher is better\n",
    "by_efficiency = sorted(\n",
    "    models,\n",
    "    key=lambda m: m[\"accuracy\"] / (m[\"train_time\"] * m[\"model_size_mb\"] / 1000),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# 6. Multi-criteria sort: First by accuracy (desc), then by size (asc)\n",
    "#    Hint: Use tuple in key function\n",
    "by_accuracy_then_size = sorted(\n",
    "    models,\n",
    "    key=lambda m: (-m[\"accuracy\"], m[\"model_size_mb\"])  # Negative for descending\n",
    ")\n",
    "\n",
    "# 7. Use filter + lambda: Get models with accuracy > 0.80 AND size < 100MB\n",
    "efficient_accurate = list(filter(\n",
    "    lambda m: # YOUR CONDITION,\n",
    "    models\n",
    "))\n",
    "\n",
    "# 8. Use map + lambda: Calculate \"performance per MB\" for all models\n",
    "performance_per_mb = list(map(\n",
    "    lambda m: # YOUR CALCULATION,\n",
    "    models\n",
    "))\n",
    "\n",
    "# Print results\n",
    "print(\"Top model by accuracy:\", by_accuracy[0][\"name\"])\n",
    "print(\"Fastest model:\", by_speed[0][\"name\"])\n",
    "print(\"Most efficient model:\", by_efficiency[0][\"name\"])\n",
    "print(\"Efficient & accurate models:\", [m[\"name\"] for m in efficient_accurate])\n",
    "print(\"Performance per MB:\", performance_per_mb)\n",
    "\n",
    "# Expected outputs:\n",
    "# Top model by accuracy: model_v3\n",
    "# Fastest model: baseline\n",
    "# Most efficient model: baseline (best accuracy/cost ratio)\n",
    "# Efficient & accurate models: ['model_v1', 'efficient']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5d40e2",
   "metadata": {},
   "source": [
    "Exercise 3: Nested Comprehensions for Data Reshaping (30 min)\n",
    "MLE Context: Reshape training data for different model inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8531b4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data: batch of sequences (e.g., text tokenized to IDs)\n",
    "sequences = [\n",
    "    [5, 12, 8, 23, 15],          # Sample 1: 5 tokens\n",
    "    [3, 18, 9],                  # Sample 2: 3 tokens\n",
    "    [7, 21, 4, 16, 11, 19],      # Sample 3: 6 tokens\n",
    "    [2, 14],                     # Sample 4: 2 tokens\n",
    "    [10, 6, 20, 13, 17, 22, 25], # Sample 5: 7 tokens\n",
    "]\n",
    "\n",
    "labels = [1, 0, 1, 0, 1]\n",
    "\n",
    "# YOUR TASK: Use comprehensions for all transformations\n",
    "\n",
    "# 1. Pad all sequences to same length (max length) with 0s\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "padded_sequences = [\n",
    "    seq + [0] * (max_len - len(seq)) for seq in sequences\n",
    "]\n",
    "\n",
    "# 2. Create attention masks: 1 where real token, 0 where padding\n",
    "masks = [\n",
    "    [1] * len(seq) + [0] * (max_len - len(seq)) for seq in sequences\n",
    "]\n",
    "\n",
    "# 3. Truncate all sequences to length 5 (cut off excess)\n",
    "truncated_sequences = [\n",
    "    seq[:5] for seq in sequences\n",
    "]\n",
    "\n",
    "# 4. Create sliding windows of size 3 from FIRST sequence\n",
    "#    E.g., [5,12,8,23,15] -> [[5,12,8], [12,8,23], [8,23,15]]\n",
    "first_seq = sequences[0]\n",
    "window_size = 3\n",
    "windows = [\n",
    "    first_seq[i:i+window_size] \n",
    "    for i in range(len(first_seq) - window_size + 1)\n",
    "]\n",
    "\n",
    "# 5. Flatten all sequences into single list\n",
    "flattened = [\n",
    "    token for seq in sequences for token in seq\n",
    "]\n",
    "\n",
    "# 6. Create vocabulary: set of all unique tokens across all sequences\n",
    "vocab = {\n",
    "    token for seq in sequences for token in seq\n",
    "}\n",
    "\n",
    "# 7. Get sequence lengths using comprehension\n",
    "seq_lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "# 8. Create batch statistics using comprehension\n",
    "batch_stats = {\n",
    "    \"min_length\": min(seq_lengths),\n",
    "    \"max_length\": max(seq_lengths),\n",
    "    \"avg_length\": sum(seq_lengths) / len(seq_lengths),\n",
    "    \"total_tokens\": sum(seq_lengths)\n",
    "}\n",
    "\n",
    "# 9. Filter: Get only sequences longer than 3 tokens with their labels\n",
    "long_sequences = [\n",
    "    (seq, label) \n",
    "    for seq, label in zip(sequences, labels) \n",
    "    if len(seq) > 3\n",
    "]\n",
    "\n",
    "# 10. Create n-grams (pairs of consecutive tokens) from all sequences\n",
    "bigrams = [\n",
    "    (seq[i], seq[i+1])\n",
    "    for seq in sequences\n",
    "    for i in range(len(seq) - 1)\n",
    "]\n",
    "\n",
    "# Print results\n",
    "print(f\"Original sequences: {sequences}\")\n",
    "print(f\"\\nPadded to length {max_len}:\")\n",
    "for i, (pad_seq, mask) in enumerate(zip(padded_sequences, masks)):\n",
    "    print(f\"  Seq {i}: {pad_seq}\")\n",
    "    print(f\"  Mask {i}: {mask}\")\n",
    "\n",
    "print(f\"\\nTruncated to length 5: {truncated_sequences}\")\n",
    "print(f\"\\nSliding windows (size 3) from first seq: {windows}\")\n",
    "print(f\"\\nFlattened: {flattened[:20]}... (first 20 tokens)\")\n",
    "print(f\"\\nVocabulary size: {len(vocab)}\")\n",
    "print(f\"Vocabulary: {sorted(vocab)}\")\n",
    "print(f\"\\nBatch statistics: {batch_stats}\")\n",
    "print(f\"\\nLong sequences (>3 tokens): {len(long_sequences)} sequences\")\n",
    "print(f\"Bigrams (first 10): {bigrams[:10]}\")\n",
    "\n",
    "# Expected outputs:\n",
    "# Padded to length 7 (all sequences same length with 0s)\n",
    "# Masks show 1s for real tokens, 0s for padding\n",
    "# Truncated: all sequences max 5 tokens\n",
    "# Windows: [[5,12,8], [12,8,23], [8,23,15]]\n",
    "# Flattened: single list with all tokens\n",
    "# Vocab size: ~26 unique tokens\n",
    "# Long sequences: 3 (sequences 0, 2, 4)\n",
    "# Bigrams: [(5,12), (12,8), (8,23), (23,15), (3,18), ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c536d673",
   "metadata": {},
   "source": [
    "DAY 7: Error Handling & Exceptions (75 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc6e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated file contents (in real scenario, these would be actual files)\n",
    "file_data = {\n",
    "    \"train_batch_1.csv\": [\n",
    "        {\"id\": 1, \"feature\": 0.5, \"label\": 1},\n",
    "        {\"id\": 2, \"feature\": 0.8, \"label\": 0},\n",
    "        {\"id\": 3, \"feature\": 0.3, \"label\": 1},\n",
    "    ],\n",
    "    \"train_batch_2.csv\": [\n",
    "        {\"id\": 4, \"feature\": \"corrupt\", \"label\": 1},  # Bad data\n",
    "        {\"id\": 5, \"feature\": 0.6, \"label\": 0},\n",
    "    ],\n",
    "    \"train_batch_3.csv\": None,  # File corrupted/empty\n",
    "    \"train_batch_4.csv\": [\n",
    "        {\"id\": 6, \"feature\": 0.9, \"label\": 0},\n",
    "        {\"id\": 7, \"feature\": 0.2, \"label\": 1},\n",
    "    ],\n",
    "}\n",
    "\n",
    "def load_batch_file(filename, data_source):\n",
    "    \"\"\"\n",
    "    Load a single batch file with error handling.\n",
    "    \n",
    "    Args:\n",
    "        filename: Name of file to load\n",
    "        data_source: Dict simulating file system\n",
    "    \n",
    "    Returns:\n",
    "        Tuple: (data_list, error_message)\n",
    "        - data_list: List of valid records, or None if file failed\n",
    "        - error_message: Error description, or None if success\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Simulate file not found\n",
    "        if filename not in data_source:\n",
    "            raise FileNotFoundError(f\"File {filename} not found\")\n",
    "        \n",
    "        # Simulate corrupted file (None content)\n",
    "        file_content = data_source[filename]\n",
    "        if file_content is None:\n",
    "            raise ValueError(f\"File {filename} is corrupted or empty\")\n",
    "        \n",
    "        # Validate data structure\n",
    "        if not isinstance(file_content, list):\n",
    "            raise TypeError(f\"File {filename} has invalid format\")\n",
    "        \n",
    "        # Validate each record\n",
    "        valid_records = []\n",
    "        for record in file_content:\n",
    "            try:\n",
    "                # Check required fields\n",
    "                if \"id\" not in record or \"feature\" not in record or \"label\" not in record:\n",
    "                    raise KeyError(f\"Record missing required fields: {record}\")\n",
    "                \n",
    "                # Validate data types\n",
    "                record_id = int(record[\"id\"])\n",
    "                feature = float(record[\"feature\"])\n",
    "                label = int(record[\"label\"])\n",
    "                \n",
    "                # Validate ranges\n",
    "                if not (0 <= feature <= 1):\n",
    "                    raise ValueError(f\"Feature {feature} out of range [0,1]\")\n",
    "                if label not in [0, 1]:\n",
    "                    raise ValueError(f\"Label {label} must be 0 or 1\")\n",
    "                \n",
    "                valid_records.append({\n",
    "                    \"id\": record_id,\n",
    "                    \"feature\": feature,\n",
    "                    \"label\": label\n",
    "                })\n",
    "                \n",
    "            except (ValueError, TypeError) as e:\n",
    "                # Skip invalid record, log error\n",
    "                print(f\"  âš ï¸  Skipping invalid record in {filename}: {record} - {e}\")\n",
    "                continue\n",
    "        \n",
    "        return valid_records, None\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        return None, f\"FileNotFoundError: {e}\"\n",
    "    except ValueError as e:\n",
    "        return None, f\"ValueError: {e}\"\n",
    "    except TypeError as e:\n",
    "        return None, f\"TypeError: {e}\"\n",
    "    except Exception as e:\n",
    "        return None, f\"UnexpectedError: {e}\"\n",
    "\n",
    "\n",
    "def load_training_data(filenames, data_source):\n",
    "    \"\"\"\n",
    "    Load all training batches with comprehensive error handling.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with keys:\n",
    "        - \"data\": Combined list of all valid records\n",
    "        - \"successful_files\": List of successfully loaded files\n",
    "        - \"failed_files\": List of (filename, error) tuples\n",
    "        - \"total_records\": Total valid records loaded\n",
    "        - \"skipped_records\": Number of records skipped due to errors\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    successful_files = []\n",
    "    failed_files = []\n",
    "    total_skipped = 0\n",
    "    \n",
    "    for filename in filenames:\n",
    "        print(f\"\\nLoading {filename}...\")\n",
    "        \n",
    "        try:\n",
    "            data, error = load_batch_file(filename, data_source)\n",
    "            \n",
    "            if error:\n",
    "                # File-level error\n",
    "                print(f\"  âŒ Failed: {error}\")\n",
    "                failed_files.append((filename, error))\n",
    "            else:\n",
    "                # Success\n",
    "                original_count = len(data_source.get(filename, []))\n",
    "                loaded_count = len(data)\n",
    "                skipped = original_count - loaded_count\n",
    "                \n",
    "                all_data.extend(data)\n",
    "                successful_files.append(filename)\n",
    "                total_skipped += skipped\n",
    "                \n",
    "                print(f\"  âœ“ Loaded {loaded_count} records (skipped {skipped})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Catch any unexpected errors\n",
    "            print(f\"  âŒ Unexpected error: {e}\")\n",
    "            failed_files.append((filename, f\"Unexpected: {e}\"))\n",
    "    \n",
    "    return {\n",
    "        \"data\": all_data,\n",
    "        \"successful_files\": successful_files,\n",
    "        \"failed_files\": failed_files,\n",
    "        \"total_records\": len(all_data),\n",
    "        \"skipped_records\": total_skipped\n",
    "    }\n",
    "\n",
    "\n",
    "# YOUR TASK: Test the loader\n",
    "filenames = [\n",
    "    \"train_batch_1.csv\",\n",
    "    \"train_batch_2.csv\",\n",
    "    \"train_batch_3.csv\",\n",
    "    \"train_batch_4.csv\",\n",
    "    \"train_batch_5.csv\",  # File doesn't exist\n",
    "]\n",
    "\n",
    "result = load_training_data(filenames, file_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LOADING SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total records loaded: {result['total_records']}\")\n",
    "print(f\"Records skipped: {result['skipped_records']}\")\n",
    "print(f\"Successful files: {len(result['successful_files'])} - {result['successful_files']}\")\n",
    "print(f\"Failed files: {len(result['failed_files'])}\")\n",
    "for filename, error in result['failed_files']:\n",
    "    print(f\"  - {filename}: {error}\")\n",
    "print(f\"\\nFinal dataset: {result['data']}\")\n",
    "\n",
    "# Expected output:\n",
    "# Loading train_batch_1.csv...\n",
    "#   âœ“ Loaded 3 records (skipped 0)\n",
    "# Loading train_batch_2.csv...\n",
    "#   âš ï¸  Skipping invalid record in train_batch_2.csv: {...} - could not convert string to float: 'corrupt'\n",
    "#   âœ“ Loaded 1 records (skipped 1)\n",
    "# Loading train_batch_3.csv...\n",
    "#   âŒ Failed: ValueError: File train_batch_3.csv is corrupted or empty\n",
    "# Loading train_batch_4.csv...\n",
    "#   âœ“ Loaded 2 records (skipped 0)\n",
    "# Loading train_batch_5.csv...\n",
    "#   âŒ Failed: FileNotFoundError: File train_batch_5.csv not found\n",
    "#\n",
    "# Total records loaded: 6\n",
    "# Successful files: 3\n",
    "# Failed files: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3afb2b",
   "metadata": {},
   "source": [
    "Exercise 2: Model Prediction with Graceful Degradation (25 min)\n",
    "MLE Context: Serve predictions even when some models fail, track errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b957d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated models (some will fail)\n",
    "class ModelSimulator:\n",
    "    def __init__(self, name, fail_mode=None):\n",
    "        self.name = name\n",
    "        self.fail_mode = fail_mode  # None, \"timeout\", \"memory\", \"invalid_output\"\n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        \"\"\"Simulate model prediction with potential failures.\"\"\"\n",
    "        # Simulate different failure modes\n",
    "        if self.fail_mode == \"timeout\":\n",
    "            raise TimeoutError(f\"{self.name}: Prediction timeout\")\n",
    "        elif self.fail_mode == \"memory\":\n",
    "            raise MemoryError(f\"{self.name}: Out of memory\")\n",
    "        elif self.fail_mode == \"invalid_output\":\n",
    "            return \"not_a_number\"  # Invalid output type\n",
    "        \n",
    "        # Successful prediction\n",
    "        import random\n",
    "        return random.uniform(0.3, 0.9)\n",
    "\n",
    "\n",
    "def predict_with_ensemble(models, input_data, strategy=\"average\"):\n",
    "    \"\"\"\n",
    "    Get predictions from multiple models with error handling.\n",
    "    \n",
    "    Args:\n",
    "        models: List of model objects\n",
    "        input_data: Input features\n",
    "        strategy: \"average\", \"max\", \"voting\", \"first_valid\"\n",
    "    \n",
    "    Returns:\n",
    "        Dict with keys:\n",
    "        - \"prediction\": Final prediction value\n",
    "        - \"successful_models\": List of models that succeeded\n",
    "        - \"failed_models\": List of (model_name, error) tuples\n",
    "        - \"confidence\": Based on how many models succeeded\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    successful_models = []\n",
    "    failed_models = []\n",
    "    \n",
    "    for model in models:\n",
    "        try:\n",
    "            # Get prediction\n",
    "            pred = model.predict(input_data)\n",
    "            \n",
    "            # Validate prediction type\n",
    "            if not isinstance(pred, (int, float)):\n",
    "                raise TypeError(f\"Invalid prediction type: {type(pred)}\")\n",
    "            \n",
    "            # Validate prediction range\n",
    "            if not (0 <= pred <= 1):\n",
    "                raise ValueError(f\"Prediction {pred} out of range [0,1]\")\n",
    "            \n",
    "            predictions.append(pred)\n",
    "            successful_models.append(model.name)\n",
    "            \n",
    "        except TimeoutError as e:\n",
    "            print(f\"  â±ï¸  {model.name}: Timeout\")\n",
    "            failed_models.append((model.name, \"Timeout\"))\n",
    "        except MemoryError as e:\n",
    "            print(f\"  ðŸ’¾ {model.name}: Out of memory\")\n",
    "            failed_models.append((model.name, \"MemoryError\"))\n",
    "        except TypeError as e:\n",
    "            print(f\"  âš ï¸  {model.name}: Invalid output type\")\n",
    "            failed_models.append((model.name, \"TypeError\"))\n",
    "        except ValueError as e:\n",
    "            print(f\"  âš ï¸  {model.name}: Invalid prediction value\")\n",
    "            failed_models.append((model.name, \"ValueError\"))\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ {model.name}: Unexpected error - {e}\")\n",
    "            failed_models.append((model.name, f\"Unexpected: {e}\"))\n",
    "    \n",
    "    # Handle case where ALL models failed\n",
    "    if len(predictions) == 0:\n",
    "        raise RuntimeError(\"All models failed - cannot make prediction\")\n",
    "    \n",
    "    # Calculate final prediction based on strategy\n",
    "    if strategy == \"average\":\n",
    "        final_pred = sum(predictions) / len(predictions)\n",
    "    elif strategy == \"max\":\n",
    "        final_pred = max(predictions)\n",
    "    elif strategy == \"first_valid\":\n",
    "        final_pred = predictions[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "    \n",
    "    # Calculate confidence based on success rate\n",
    "    total_models = len(models)\n",
    "    success_rate = len(successful_models) / total_models\n",
    "    \n",
    "    return {\n",
    "        \"prediction\": final_pred,\n",
    "        \"successful_models\": successful_models,\n",
    "        \"failed_models\": failed_models,\n",
    "        \"confidence\": success_rate,\n",
    "        \"num_predictions\": len(predictions)\n",
    "    }\n",
    "\n",
    "\n",
    "# YOUR TASK: Test the ensemble predictor\n",
    "\n",
    "models = [\n",
    "    ModelSimulator(\"model_a\", fail_mode=None),          # Works fine\n",
    "    ModelSimulator(\"model_b\", fail_mode=\"timeout\"),     # Fails with timeout\n",
    "    ModelSimulator(\"model_c\", fail_mode=None),          # Works fine\n",
    "    ModelSimulator(\"model_d\", fail_mode=\"memory\"),      # Fails with memory error\n",
    "    ModelSimulator(\"model_e\", fail_mode=None),          # Works fine\n",
    "    ModelSimulator(\"model_f\", fail_mode=\"invalid_output\"),  # Returns bad output\n",
    "]\n",
    "\n",
    "input_data = {\"feature1\": 0.5, \"feature2\": 0.8}\n",
    "\n",
    "print(\"Making ensemble prediction...\\n\")\n",
    "\n",
    "try:\n",
    "    result = predict_with_ensemble(models, input_data, strategy=\"average\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PREDICTION RESULT\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Final prediction: {result['prediction']:.4f}\")\n",
    "    print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "    print(f\"Successful models ({result['num_predictions']}): {result['successful_models']}\")\n",
    "    print(f\"Failed models ({len(result['failed_models'])}):\")\n",
    "    for model_name, error in result['failed_models']:\n",
    "        print(f\"  - {model_name}: {error}\")\n",
    "    \n",
    "    # Determine if prediction is reliable\n",
    "    if result['confidence'] < 0.5:\n",
    "        print(\"\\nâš ï¸  WARNING: Low confidence - less than 50% of models succeeded\")\n",
    "    elif result['confidence'] < 0.7:\n",
    "        print(\"\\nâš ï¸  CAUTION: Medium confidence - consider retrying\")\n",
    "    else:\n",
    "        print(\"\\nâœ“ HIGH CONFIDENCE: Majority of models succeeded\")\n",
    "        \n",
    "except RuntimeError as e:\n",
    "    print(f\"\\nâŒ CRITICAL ERROR: {e}\")\n",
    "    print(\"Cannot serve prediction - all models failed\")\n",
    "\n",
    "# Expected output:\n",
    "# model_b: Timeout\n",
    "# model_d: Out of memory  \n",
    "# model_f: Invalid output type\n",
    "# \n",
    "# Final prediction: 0.XXXX (average of 3 successful predictions)\n",
    "# Confidence: 50% (3 out of 6 models)\n",
    "# Successful models: [model_a, model_c, model_e]\n",
    "# Failed models: [model_b: Timeout, model_d: MemoryError, model_f: TypeError]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5adb55",
   "metadata": {},
   "source": [
    "Exercise 3: Data Pipeline with Retry Logic (25 min)\n",
    "MLE Context: Fetch data from API with retries, handle transient failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dc7723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class APISimulator:\n",
    "    \"\"\"Simulates an unreliable API.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.call_count = 0\n",
    "    \n",
    "    def fetch_data(self, endpoint):\n",
    "        \"\"\"Simulate API call with intermittent failures.\"\"\"\n",
    "        self.call_count += 1\n",
    "        \n",
    "        # Fail on first 2 calls, succeed on 3rd (simulating transient error)\n",
    "        if self.call_count <= 2:\n",
    "            import random\n",
    "            error_type = random.choice([\"timeout\", \"server_error\", \"rate_limit\"])\n",
    "            \n",
    "            if error_type == \"timeout\":\n",
    "                raise TimeoutError(\"API timeout\")\n",
    "            elif error_type == \"server_error\":\n",
    "                raise ConnectionError(\"500 Internal Server Error\")\n",
    "            elif error_type == \"rate_limit\":\n",
    "                raise Exception(\"429 Rate Limit Exceeded\")\n",
    "        \n",
    "        # Successful response\n",
    "        return {\n",
    "            \"data\": [\n",
    "                {\"id\": 1, \"value\": 0.5},\n",
    "                {\"id\": 2, \"value\": 0.8},\n",
    "                {\"id\": 3, \"value\": 0.3},\n",
    "            ],\n",
    "            \"status\": \"success\"\n",
    "        }\n",
    "\n",
    "\n",
    "def fetch_with_retry(api, endpoint, max_retries=3, backoff_factor=1):\n",
    "    \"\"\"\n",
    "    Fetch data from API with exponential backoff retry logic.\n",
    "    \n",
    "    Args:\n",
    "        api: API object\n",
    "        endpoint: Endpoint to call\n",
    "        max_retries: Maximum number of retry attempts\n",
    "        backoff_factor: Base wait time (doubles each retry)\n",
    "    \n",
    "    Returns:\n",
    "        API response data\n",
    "    \n",
    "    Raises:\n",
    "        Exception: If all retries exhausted\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    last_error = None\n",
    "    \n",
    "    while attempt <= max_retries:\n",
    "        try:\n",
    "            print(f\"Attempt {attempt + 1}/{max_retries + 1}: Fetching {endpoint}...\")\n",
    "            \n",
    "            response = api.fetch_data(endpoint)\n",
    "            \n",
    "            # Validate response\n",
    "            if not isinstance(response, dict):\n",
    "                raise TypeError(\"Invalid response format\")\n",
    "            \n",
    "            if \"data\" not in response:\n",
    "                raise ValueError(\"Response missing 'data' field\")\n",
    "            \n",
    "            print(f\"  âœ“ Success on attempt {attempt + 1}\")\n",
    "            return response[\"data\"]\n",
    "            \n",
    "        except TimeoutError as e:\n",
    "            last_error = e\n",
    "            print(f\"  â±ï¸  Timeout error: {e}\")\n",
    "            \n",
    "        except ConnectionError as e:\n",
    "            last_error = e\n",
    "            print(f\"  ðŸ”Œ Connection error: {e}\")\n",
    "            \n",
    "        except ValueError as e:\n",
    "            last_error = e\n",
    "            print(f\"  âš ï¸  Value error: {e}\")\n",
    "            # Don't retry on validation errors\n",
    "            raise\n",
    "            \n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            print(f\"  âŒ Unexpected error: {e}\")\n",
    "        \n",
    "        # Increment attempt counter\n",
    "        attempt += 1\n",
    "        \n",
    "        # If not the last attempt, wait before retrying (exponential backoff)\n",
    "        if attempt <= max_retries:\n",
    "            wait_time = backoff_factor * (2 ** (attempt - 1))\n",
    "            print(f\"  Retrying in {wait_time}s...\\n\")\n",
    "            time.sleep(wait_time)\n",
    "        else:\n",
    "            # All retries exhausted\n",
    "            print(f\"  âŒ All {max_retries + 1} attempts failed\")\n",
    "            raise RuntimeError(f\"Failed after {max_retries + 1} attempts. Last error: {last_error}\")\n",
    "\n",
    "\n",
    "def load_dataset_from_api(endpoints):\n",
    "    \"\"\"\n",
    "    Load data from multiple API endpoints with error handling.\n",
    "    \n",
    "    Returns:\n",
    "        Dict with combined data and statistics\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    successful_endpoints = []\n",
    "    failed_endpoints = []\n",
    "    \n",
    "    for endpoint in endpoints:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Loading {endpoint}\")\n",
    "        print('='*50)\n",
    "        \n",
    "        try:\n",
    "            data = fetch_with_retry(\n",
    "                APISimulator(),  # Fresh API instance for each endpoint\n",
    "                endpoint,\n",
    "                max_retries=3,\n",
    "                backoff_factor=0.5  # Fast for testing (0.5s, 1s, 2s)\n",
    "            )\n",
    "            \n",
    "            all_data.extend(data)\n",
    "            successful_endpoints.append(endpoint)\n",
    "            print(f\"âœ“ Loaded {len(data)} records from {endpoint}\\n\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"âŒ Failed to load {endpoint}: {e}\\n\")\n",
    "            failed_endpoints.append((endpoint, str(e)))\n",
    "            \n",
    "        except ValueError as e:\n",
    "            print(f\"âŒ Invalid data from {endpoint}: {e}\\n\")\n",
    "            failed_endpoints.append((endpoint, str(e)))\n",
    "    \n",
    "    return {\n",
    "        \"data\": all_data,\n",
    "        \"successful\": successful_endpoints,\n",
    "        \"failed\": failed_endpoints,\n",
    "        \"total_records\": len(all_data)\n",
    "    }\n",
    "\n",
    "\n",
    "# YOUR TASK: Test the retry logic\n",
    "\n",
    "endpoints = [\n",
    "    \"/api/train_data_1\",\n",
    "    \"/api/train_data_2\",\n",
    "    \"/api/train_data_3\",\n",
    "]\n",
    "\n",
    "result = load_dataset_from_api(endpoints)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATASET LOADING SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total records: {result['total_records']}\")\n",
    "print(f\"Successful endpoints: {len(result['successful'])} - {result['successful']}\")\n",
    "print(f\"Failed endpoints: {len(result['failed'])}\")\n",
    "for endpoint, error in result['failed']:\n",
    "    print(f\"  - {endpoint}: {error}\")\n",
    "\n",
    "# Expected output:\n",
    "# Each endpoint will fail 1-2 times before succeeding on retry\n",
    "# Exponential backoff: 0.5s, 1s, 2s between retries\n",
    "# All endpoints should eventually succeed (simulated transient failures)\n",
    "# Total records: 9 (3 endpoints Ã— 3 records each)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
